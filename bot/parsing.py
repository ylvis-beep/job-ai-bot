# parsing.py
import logging
import re
from io import BytesIO

import requests
from bs4 import BeautifulSoup
from PyPDF2 import PdfReader

from config import PROXY_URL, MIN_MEANINGFUL_TEXT_LENGTH

logger = logging.getLogger(__name__)

# =========================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –¢–ï–ö–°–¢–ê
# =========================

def clean_text(raw: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞: —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏."""
    if not raw:
        return ""

    text = raw.replace('\r\n', '\n').replace('\r', '\n')
    lines = [line.strip() for line in text.split('\n')]
    text = '\n'.join(lines)
    text = re.sub(r'\n{3,}', '\n\n', text)

    return text.strip()


# =========================
# PDF –ü–ê–†–°–ï–†
# =========================

def extract_text_from_pdf_bytes(data: bytes) -> str:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–∞.
    """
    try:
        reader = PdfReader(BytesIO(data))
        pages_text = []

        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                pages_text.append(page_text)

        text = "\n\n".join(pages_text)
        text = clean_text(text)

        if not text or len(text) < 50:
            raise ValueError("PDF —Ñ–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Ç–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")

        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ PDF")
        return text

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {str(e)}", exc_info=True)
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å PDF —Ñ–∞–π–ª: {str(e)}")


# =========================
# –õ–û–ì–ò–ö–ê –†–ê–ë–û–¢–´ –°–û –°–°–´–õ–ö–ê–ú–ò
# =========================

URL_REGEX = re.compile(
    r'^(https?://)?([a-z0-9.-]+\.[a-z]{2,})(/.*)?$',
    re.IGNORECASE
)


def looks_like_url(text: str) -> bool:
    """
    –ú—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ ‚Äì –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ URL.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã:
    - https://hh.ru/vacancy/123
    - http://example.com
    - hh.ru/vacancy/123
    - www.hh.ru/vacancy/123
    """
    if not text:
        return False
    text = text.strip()
    return bool(URL_REGEX.match(text))


def normalize_url(text: str) -> str:
    """
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ URL –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å http/https.
    'hh.ru/vacancy/123' -> 'https://hh.ru/vacancy/123'
    """
    text = text.strip()
    if not text.startswith(("http://", "https://")):
        return "https://" + text
    return text


def html_to_text(html: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML."""
    try:
        soup = BeautifulSoup(html, 'html.parser')

        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in soup(["script", "style", "nav", "footer", "header"]):
            element.decompose()

        text = soup.get_text(separator='\n')
        return clean_text(text)

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML: {str(e)}", exc_info=True)
        return ""


# =========================
# –ü–†–û–ö–°–ò-–ü–ê–†–°–ò–ù–ì –ß–ï–†–ï–ó RU-–ü–†–û–ö–°–ò
# =========================

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
}


def fetch_html_via_proxy(url: str) -> str:
    """
    –ó–∞–ø—Ä–æ—Å HTML —á–µ—Ä–µ–∑ RU-–ø—Ä–æ–∫—Å–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Bright Data).
    PROXY_URL –∑–∞–¥–∞—ë—Ç—Å—è –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä:
    PROXY_URL=http://user:pass@brd.superproxy.io:33335
    """
    if not PROXY_URL:
        raise ValueError(
            "‚ùå PROXY_URL –Ω–µ –∑–∞–¥–∞–Ω.\n"
            "–ó–∞–¥–∞–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è PROXY_URL —Å –∞–¥—Ä–µ—Å–æ–º –ø—Ä–æ–∫—Å–∏, "
            "–Ω–∞–ø—Ä–∏–º–µ—Ä: http://user:pass@host:port"
        )

    proxies = {
        "http": PROXY_URL,
        "https": PROXY_URL,
    }

    try:
        logger.info(f"üîó –ü–∞—Ä—Å–∏–º —Å—Å—ã–ª–∫—É —á–µ—Ä–µ–∑ RU-–ø—Ä–æ–∫—Å–∏: {url}")

        resp = requests.get(
            url,
            headers=HEADERS,
            proxies=proxies,
            timeout=30,
        )

        logger.info(f"–ü—Ä–æ–∫—Å–∏ –æ—Ç–≤–µ—Ç–∏–ª —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º: {resp.status_code}")

        resp.raise_for_status()
        html = resp.text

        # –ü—Ä–æ—Å—Ç–µ–π—à–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∞–ø—á—É/–±–ª–æ–∫–∏—Ä–æ–≤–∫—É
        lower = html.lower()
        if any(m in lower for m in ["captcha", "access denied", "are you human"]):
            logger.warning("‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –∫–∞–ø—á–µ–π/–±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π")
            # –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å—Ä–∞–∑—É –ø–∞–¥–∞—Ç—å, –Ω–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ –º–æ–∂–Ω–æ —Ç–∞–∫:
            raise ValueError("–°–∞–π—Ç –≤–µ—Ä–Ω—É–ª –∫–∞–ø—á—É/–±–ª–æ–∫–∏—Ä–æ–≤–∫—É")

        if len(html) < 500:
            logger.warning(f"‚ö†Ô∏è –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç ({len(html)} —Å–∏–º–≤–æ–ª–æ–≤)")
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å —Å–∞–π—Ç–∞.")

        return html

    except requests.exceptions.Timeout:
        logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}", exc_info=True)
        raise TimeoutError("–°–∞–π—Ç –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {str(e)}", exc_info=True)
        raise ValueError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Å—ã–ª–∫–∏: {str(e)}")


def fetch_url_text_via_proxy(url: str) -> str:
    """
    –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –ø–æ–ª—É—á–∞–µ–º HTML —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏,
    –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ–Ω –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π.
    """
    html = fetch_html_via_proxy(url)
    text = html_to_text(html)

    if not text or len(text) < MIN_MEANINGFUL_TEXT_LENGTH:
        raise ValueError(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç —Å —Å–∞–π—Ç–∞.\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –≤—Ä—É—á–Ω—É—é."
        )

    logger.info(f"‚úÖ –°—Å—ã–ª–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞, –ø–æ–ª—É—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞")
    return text

