import logging
import re
from io import BytesIO

import requests
from bs4 import BeautifulSoup
from PyPDF2 import PdfReader

from config import PROXY_URL, MIN_MEANINGFUL_TEXT_LENGTH

logger = logging.getLogger(__name__)

# –ï–¥–∏–Ω–æ–µ "—á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ" —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é,
# –∫–æ–≥–¥–∞ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–∫–∞–Ω—Å–∏—é –ø–æ —Å—Å—ã–ª–∫–µ
GENERIC_VACANCY_ERROR_MSG = (
    "–ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —Å–∞–π—Ç–∞.\n"
    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –≤—Ä—É—á–Ω—É—é."
)

# =========================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –¢–ï–ö–°–¢–ê
# =========================


def clean_text(raw: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞: —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏."""
    if not raw:
        return ""

    text = raw.replace("\r\n", "\n").replace("\r", "\n")
    lines = [line.strip() for line in text.split("\n")]
    text = "\n".join(lines)
    text = re.sub(r"\n{3,}", "\n\n", text)

    return text.strip()


# =========================
# PDF –ü–ê–†–°–ï–†
# =========================


def extract_text_from_pdf_bytes(data: bytes) -> str:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–∞.
    """
    try:
        reader = PdfReader(BytesIO(data))
        pages_text = []

        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                pages_text.append(page_text)

        text = "\n\n".join(pages_text)
        text = clean_text(text)

        if not text or len(text) < 50:
            raise ValueError("PDF —Ñ–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Ç–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")

        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ PDF")
        return text

    except ValueError:
        # –£–∂–µ ¬´—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è¬ª –æ—à–∏–±–∫–∞, –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–∫–∏–¥—ã–≤–∞–µ–º –≤—ã—à–µ
        raise

    except Exception as e:
        # –í –ª–æ–≥ –ø–∏—à–µ–º –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é ‚Äî –∞–∫–∫—É—Ä–∞—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}", exc_info=True)
        raise ValueError(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å PDF —Ñ–∞–π–ª. "
            "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –ø–æ–≤—Ä–µ–∂–¥—ë–Ω –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."
        )


# =========================
# –õ–û–ì–ò–ö–ê –†–ê–ë–û–¢–´ –°–û –°–°–´–õ–ö–ê–ú–ò
# =========================

URL_REGEX = re.compile(
    r"^(https?://)?([a-z0-9.-]+\.[a-z]{2,})(/.*)?$",
    re.IGNORECASE,
)


def looks_like_url(text: str) -> bool:
    """
    –ú—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ ‚Äì –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ URL.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã:
    - https://hh.ru/vacancy/123
    - http://example.com
    - hh.ru/vacancy/123
    - www.hh.ru/vacancy/123
    """
    if not text:
        return False
    text = text.strip()
    return bool(URL_REGEX.match(text))


def normalize_url(text: str) -> str:
    """
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ URL –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å http/https.
    'hh.ru/vacancy/123' -> 'https://hh.ru/vacancy/123'
    """
    text = text.strip()
    if not text.startswith(("http://", "https://")):
        return "https://" + text
    return text


def html_to_text(html: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML."""
    try:
        soup = BeautifulSoup(html, "html.parser")

        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in soup(["script", "style", "nav", "footer", "header"]):
            element.decompose()

        text = soup.get_text(separator="\n")
        return clean_text(text)

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML: {e}", exc_info=True)
        return ""


# =========================
# –ü–†–û–ö–°–ò-–ü–ê–†–°–ò–ù–ì –ß–ï–†–ï–ó RU-–ü–†–û–ö–°–ò
# =========================

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
}


def _normalize_proxy_url(raw: str) -> str:
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç PROXY_URL –∫ –≤–∏–¥—É, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–Ω–∏–º–∞–µ—Ç requests.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç—ã:
    - http://user:pass@host:port
    - socks5://user:pass@host:port
    - host:port@user:pass      (–∫–∞–∫ –¥–∞—ë—Ç proxy.market)
    - user:pass@host:port
    - host:port
    """
    raw = (raw or "").strip()
    if not raw:
        return raw

    # –£–∂–µ –µ—Å—Ç—å —Å—Ö–µ–º–∞ (http://, https://, socks5:// –∏ —Ç.–ø.)
    if re.match(r"^[a-zA-Z0-9+.-]+://", raw):
        return raw

    # –ï—Å–ª–∏ –µ—Å—Ç—å –ª–æ–≥–∏–Ω/–ø–∞—Ä–æ–ª—å –∏ —Ö–æ—Å—Ç, –Ω–æ –≤ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
    if "@" in raw:
        left, right = raw.split("@", 1)

        def looks_like_host_port(part: str) -> bool:
            # –ü—Ä–∏–º–∏—Ç–∏–≤–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –≤ —Ö–æ—Å—Ç–µ –æ–±—ã—á–Ω–æ –µ—Å—Ç—å —Ç–æ—á–∫–∞ –∏ –±—É–∫–≤—ã
            host, _, _ = part.partition(":")
            return "." in host and re.search(r"[a-zA-Z]", host) is not None

        if looks_like_host_port(left):
            host_port = left
            creds = right
        else:
            creds = left
            host_port = right

        return f"http://{creds}@{host_port}"

    # –ü—Ä–æ—Å—Ç–æ host:port ‚Äî –±–µ–∑ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    return f"http://{raw}"


def fetch_html_via_proxy(url: str) -> str:
    """
    –ó–∞–ø—Ä–æ—Å HTML —á–µ—Ä–µ–∑ RU-–ø—Ä–æ–∫—Å–∏.
    PROXY_URL –º–æ–∂–µ—Ç –±—ã—Ç—å:
    - http://user:pass@host:port
    - host:port@user:pass (–∫–∞–∫ —É proxy.market)
    - –∏ –¥—Ä. –≤–∞—Ä–∏–∞–Ω—Ç—ã, –æ–ø–∏—Å–∞–Ω–Ω—ã–µ –≤ _normalize_proxy_url.
    """
    if not PROXY_URL:
        raise ValueError(
            "PROXY_URL –Ω–µ –∑–∞–¥–∞–Ω. "
            "–ó–∞–¥–∞–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è PROXY_URL —Å –∞–¥—Ä–µ—Å–æ–º –ø—Ä–æ–∫—Å–∏, "
            "–Ω–∞–ø—Ä–∏–º–µ—Ä: pool.proxy.market:10000@login:password"
        )

    proxy_url = _normalize_proxy_url(PROXY_URL)
    proxies = {
        "http": proxy_url,
        "https": proxy_url,
    }

    try:
        logger.info(f"üîó –ü–∞—Ä—Å–∏–º —Å—Å—ã–ª–∫—É —á–µ—Ä–µ–∑ RU-–ø—Ä–æ–∫—Å–∏: {url}")

        resp = requests.get(
            url,
            headers=HEADERS,
            proxies=proxies,
            timeout=30,
        )

        logger.info(f"–ü—Ä–æ–∫—Å–∏ –æ—Ç–≤–µ—Ç–∏–ª —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º: {resp.status_code}")
        resp.raise_for_status()

        html = resp.text

        # –ü—Ä–æ—Å—Ç–µ–π—à–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∞–ø—á—É/–±–ª–æ–∫–∏—Ä–æ–≤–∫—É
        lower = html.lower()
        if any(m in lower for m in ["captcha", "access denied", "are you human"]):
            logger.warning("‚ö†Ô∏è –ü–æ—Ö–æ–∂–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –∫–∞–ø—á–µ–π/–±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π")
            # –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –≥–æ–≤–æ—Ä–∏–º –ø—Ä–æ—Å—Ç–æ ¬´–Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏¬ª
            raise ValueError(GENERIC_VACANCY_ERROR_MSG)

        if len(html) < 500:
            logger.warning(f"‚ö†Ô∏è –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç ({len(html)} —Å–∏–º–≤–æ–ª–æ–≤)")
            raise ValueError(GENERIC_VACANCY_ERROR_MSG)

        return html

    except ValueError:
        # –£–∂–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –æ—à–∏–±–∫–∞, –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º
        raise

    except requests.exceptions.Timeout:
        logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}", exc_info=True)
        raise ValueError(GENERIC_VACANCY_ERROR_MSG)

    except requests.exceptions.ProxyError as e:
        # –ó–¥–µ—Å—å –∫–∞–∫ —Ä–∞–∑ –±—É–¥—É—Ç –æ—à–∏–±–∫–∏ –≤–∏–¥–∞ 407 Proxy Authentication Required
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ø—Ä–æ–∫—Å–∏: {e}", exc_info=True)
        raise ValueError(GENERIC_VACANCY_ERROR_MSG)

    except requests.RequestException as e:
        # –õ—é–±—ã–µ –ø—Ä–æ—á–∏–µ —Å–µ—Ç–µ–≤—ã–µ/HTTP-–æ—à–∏–±–∫–∏
        logger.error(f"‚ùå HTTP/—Å–µ—Ç–µ–≤–æ–π —Å–±–æ–π –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {e}", exc_info=True)
        raise ValueError(GENERIC_VACANCY_ERROR_MSG)

    except Exception as e:
        logger.error(f"‚ùå –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {e}", exc_info=True)
        raise ValueError(GENERIC_VACANCY_ERROR_MSG)


def fetch_url_text_via_proxy(url: str) -> str:
    """
    –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –ø–æ–ª—É—á–∞–µ–º HTML —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏,
    –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ–Ω –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π.
    """
    html = fetch_html_via_proxy(url)
    text = html_to_text(html)

    if not text or len(text) < MIN_MEANINGFUL_TEXT_LENGTH:
        logger.warning(
            f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –æ–±—ä—ë–º —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)"
        )
        raise ValueError(GENERIC_VACANCY_ERROR_MSG)

    logger.info(f"‚úÖ –°—Å—ã–ª–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞, –ø–æ–ª—É—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞")
    return text
