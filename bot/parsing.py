import cloudscraper
from fake_useragent import UserAgent
import browser_cookie3
import json

# =========================
# CLOUDSCRAPER - –û–ë–•–û–î CLOUDFLARE (–∏–∑ —Å—Ç–∞—Ç—å–∏)
# =========================

def create_cloudscraper_session(proxy_url: Optional[str] = None):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ cloudscraper –¥–ª—è –æ–±—Ö–æ–¥–∞ Cloudflare.
    –û–ø–∏—Å–∞–Ω –≤ —Å—Ç–∞—Ç—å–µ –∫–∞–∫ –æ–¥–∏–Ω –∏–∑ –ª—É—á—à–∏—Ö –º–µ—Ç–æ–¥–æ–≤.
    """
    try:
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False,
                'desktop': True,
            }
        )
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–∫—Å–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        if proxy_url:
            normalized_proxy = _normalize_proxy_url(proxy_url)
            scraper.proxies = {
                'http': normalized_proxy,
                'https': normalized_proxy
            }
        
        # –ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∫–∞–∫ —É —Ä–µ–∞–ª—å–Ω–æ–≥–æ –±—Ä–∞—É–∑–µ—Ä–∞
        ua = UserAgent()
        headers = {
            'User-Agent': ua.chrome,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'TE': 'Trailers',
        }
        
        scraper.headers.update(headers)
        
        # –î–æ–±–∞–≤–ª—è–µ–º cookies –æ—Ç Chrome (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
        try:
            chrome_cookies = browser_cookie3.chrome(domain_name='.tochka.com')
            for cookie in chrome_cookies:
                scraper.cookies.set_cookie(cookie)
        except:
            pass
        
        return scraper
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è cloudscraper: {e}")
        raise

def parse_with_cloudscraper(url: str, proxy_url: Optional[str] = None) -> str:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Cloudscraper - –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∏–∑ —Å—Ç–∞—Ç—å–∏.
    –û–±—Ö–æ–¥–∏—Ç Cloudflare, DDoS-GUARD –∏ –ø–æ–¥–æ–±–Ω—ã–µ –∑–∞—â–∏—Ç—ã.
    """
    try:
        logger.info(f"‚òÅÔ∏è Cloudscraper: –ø–∞—Ä—Å–∏–º {url}")
        
        scraper = create_cloudscraper_session(proxy_url)
        
        # –ò–º–∏—Ç–∏—Ä—É–µ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
        time.sleep(random.uniform(1, 3))
        
        response = scraper.get(url, timeout=30)
        
        if response.status_code == 403:
            logger.warning("‚ö†Ô∏è Cloudscraper –ø–æ–ª—É—á–∏–ª 403 - –ø–æ–ø—Ä–æ–±—É–µ–º —Å –¥—Ä—É–≥–∏–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏")
            # –ü—Ä–æ–±—É–µ–º —Å –¥—Ä—É–≥–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
            return _retry_with_different_headers(url, proxy_url)
        
        if response.status_code != 200:
            raise ValueError(f"Cloudscraper –æ—à–∏–±–∫–∞: {response.status_code}")
        
        html = response.text
        
        if detect_captcha(html):
            logger.warning("‚ö†Ô∏è Cloudscraper: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞")
            raise ValueError(GENERIC_VACANCY_ERROR_MSG)
        
        logger.info(f"‚úÖ Cloudscraper —É—Å–ø–µ—à–Ω–æ: {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
        return html
        
    except Exception as e:
        logger.error(f"‚ùå Cloudscraper –æ—à–∏–±–∫–∞: {e}")
        raise

def _retry_with_different_headers(url: str, proxy_url: Optional[str] = None) -> str:
    """–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å –¥—Ä—É–≥–∏–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏"""
    try:
        scraper = cloudscraper.create_scraper(
            interpreter='nodejs',  # –ü—Ä–æ–±—É–µ–º Node.js –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä
            delay=10  # –ó–∞–¥–µ—Ä–∂–∫–∞ –∫–∞–∫ —É —á–µ–ª–æ–≤–µ–∫–∞
        )
        
        if proxy_url:
            normalized_proxy = _normalize_proxy_url(proxy_url)
            scraper.proxies = {'http': normalized_proxy, 'https': normalized_proxy}
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        ua = UserAgent()
        headers = {
            'User-Agent': ua.firefox,  # –ü—Ä–æ–±—É–µ–º Firefox
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
        }
        
        scraper.headers.update(headers)
        time.sleep(random.uniform(2, 4))
        
        response = scraper.get(url, timeout=30)
        return response.text if response.status_code == 200 else ""
        
    except Exception as e:
        logger.error(f"‚ùå Retry with headers failed: {e}")
        raise

# =========================
# –£–õ–£–ß–®–ï–ù–ù–´–ô REQUESTS –° –ë–†–ê–£–ó–ï–†–ù–´–ú–ò –ó–ê–ì–û–õ–û–í–ö–ê–ú–ò
# =========================

def get_browser_headers() -> dict:
    """–ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∫–∞–∫ —É —Ä–µ–∞–ª—å–Ω–æ–≥–æ –±—Ä–∞—É–∑–µ—Ä–∞ (–∏–∑ —Å—Ç–∞—Ç—å–∏)"""
    ua = UserAgent()
    
    return {
        'User-Agent': ua.chrome,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'sec-ch-ua': '"Google Chrome";v="119", "Chromium";v="119", "Not?A_Brand";v="24"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'TE': 'trailers',
    }

def create_requests_session_with_cookies(proxy_url: Optional[str] = None):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ requests —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º cookies –∏ –±—Ä–∞—É–∑–µ—Ä–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏.
    –í–∞–∂–Ω–æ –¥–ª—è —Å–∞–π—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª–µ–¥—è—Ç –∑–∞ —Å–µ—Å—Å–∏—è–º–∏.
    """
    session = requests.Session()
    
    # –ü–æ–ª–Ω—ã–µ –±—Ä–∞—É–∑–µ—Ä–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    session.headers.update(get_browser_headers())
    
    # –ü—Ä–æ–∫—Å–∏
    if proxy_url:
        normalized_proxy = _normalize_proxy_url(proxy_url)
        session.proxies = {
            'http': normalized_proxy,
            'https': normalized_proxy
        }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º cookies –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
    session.cookies.update(requests.cookies.RequestsCookieJar())
    
    return session

# =========================
# –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –£–ú–ù–´–ô –ü–ê–†–°–ï–† –° –ü–†–ò–û–†–ò–¢–ï–¢–ê–ú–ò –ò–ó –°–¢–ê–¢–¨–ò
# =========================

def fetch_url_text_via_proxy(url: str) -> str:
    """
    –£–º–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –ø–æ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ –∏–∑ —Å—Ç–∞—Ç—å–∏:
    1. Cloudscraper + –ø—Ä–æ–∫—Å–∏ (–ª—É—á—à–∏–π –¥–ª—è Cloudflare)
    2. Selenium + –ø—Ä–æ–∫—Å–∏ (–¥–ª—è JS-—Å–∞–π—Ç–æ–≤)
    3. Requests —Å –±—Ä–∞—É–∑–µ—Ä–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ + –ø—Ä–æ–∫—Å–∏
    4. Requests –±–µ–∑ –ø—Ä–æ–∫—Å–∏
    """
    methods_to_try = []
    
    # 1. Cloudscraper + –ø—Ä–æ–∫—Å–∏ (–û–°–ù–û–í–ù–û–ô –∏–∑ —Å—Ç–∞—Ç—å–∏)
    if PROXY_URL:
        methods_to_try.append(
            ("Cloudscraper —Å –ø—Ä–æ–∫—Å–∏", 
             lambda: parse_with_cloudscraper(url, PROXY_URL))
        )
    
    # 2. Selenium + –ø—Ä–æ–∫—Å–∏ (–¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö JS-—Å–∞–π—Ç–æ–≤)
    if SELENIUM_ENABLED and PROXY_URL:
        methods_to_try.append(
            ("Selenium —Å –ø—Ä–æ–∫—Å–∏", 
             lambda: parse_with_selenium(url, PROXY_URL))
        )
    
    # 3. Requests —Å –ø–æ–ª–Ω—ã–º–∏ –±—Ä–∞—É–∑–µ—Ä–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ + –ø—Ä–æ–∫—Å–∏
    if PROXY_URL:
        methods_to_try.append(
            ("Requests —Å –±—Ä–∞—É–∑–µ—Ä–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏", 
             lambda: _parse_with_browser_headers(url, PROXY_URL))
        )
    
    # 4. Requests –±–µ–∑ –ø—Ä–æ–∫—Å–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–∞—Ä–∏–∞–Ω—Ç)
    methods_to_try.append(
        ("Requests –±–µ–∑ –ø—Ä–æ–∫—Å–∏", 
         lambda: _parse_with_browser_headers(url, None))
    )
    
    last_error = None
    
    for method_name, parser_func in methods_to_try:
        try:
            logger.info(f"üîÑ –ü—Ä–æ–±—É–µ–º {method_name} –¥–ª—è {url}")
            
            # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ (–∏–º–∏—Ç–∞—Ü–∏—è —á–µ–ª–æ–≤–µ–∫–∞)
            if method_name != methods_to_try[0][0]:  # –ù–µ –¥–ª—è –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–∏
                delay = random.uniform(2, 5)
                time.sleep(delay)
            
            html = parser_func()
            text = html_to_text(html)
            
            if text and len(text) >= MIN_MEANINGFUL_TEXT_LENGTH:
                logger.info(f"‚úÖ {method_name} —É—Å–ø–µ—à–µ–Ω: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                return text
            else:
                logger.warning(f"‚ö†Ô∏è {method_name}: –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞")
                last_error = ValueError(GENERIC_VACANCY_ERROR_MSG)
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è {method_name} –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
            last_error = e
            continue
    
    # –í—Å–µ –º–µ—Ç–æ–¥—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏
    logger.error(f"‚ùå –í—Å–µ –º–µ—Ç–æ–¥—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–ª—è {url}")
    raise ValueError(GENERIC_VACANCY_ERROR_MSG)

def _parse_with_browser_headers(url: str, proxy_url: Optional[str] = None) -> str:
    """–ü–∞—Ä—Å–∏–Ω–≥ —Å –ø–æ–ª–Ω—ã–º–∏ –±—Ä–∞—É–∑–µ—Ä–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏"""
    session = create_requests_session_with_cookies(proxy_url)
    
    try:
        # –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ cookies
        session.get('https://google.com', timeout=5)
        time.sleep(random.uniform(1, 2))
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å
        response = session.get(url, timeout=20)
        response.raise_for_status()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–∞–ø—á—É
        if detect_captcha(response.text):
            raise ValueError("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞")
        
        return response.text
        
    except Exception as e:
        logger.error(f"‚ùå Browser headers parse failed: {e}")
        raise

# =========================
# –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û: –û–ë–†–ê–ë–û–¢–ö–ê 403 –ò –ö–ê–ü–ß–ò
# =========================

def is_blocked_response(html: str, status_code: int) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –ª–∏ –Ω–∞—Å —Å–∞–π—Ç"""
    if status_code == 403:
        return True
    
    if not html:
        return True
    
    html_lower = html.lower()
    
    block_indicators = [
        "access denied",
        "forbidden",
        "blocked",
        "bot detected",
        "security check",
        "—Ä–∞–±–æ—Ç–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞",
        "–¥–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â—ë–Ω",
        "–≤–∞—à ip-–∞–¥—Ä–µ—Å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω",
    ]
    
    return any(indicator in html_lower for indicator in block_indicators)

def rotate_user_agent():
    """–†–æ—Ç–∞—Ü–∏—è User-Agent –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
    ua = UserAgent()
    return {
        'chrome': ua.chrome,
        'firefox': ua.firefox,
        'safari': ua.safari,
        'random': ua.random,
    }
