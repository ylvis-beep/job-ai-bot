import logging
import re
import time
import random
import os
from io import BytesIO
from typing import Optional, Tuple
from urllib.parse import urlparse

import requests
from bs4 import BeautifulSoup
from PyPDF2 import PdfReader

from config import (
    PROXY_URL, 
    MIN_MEANINGFUL_TEXT_LENGTH,
    CLOUDSCRAPER_ENABLED,
    FORCE_MOBILE_HH,
    RETRY_COUNT,
    IS_RENDER
)

logger = logging.getLogger(__name__)

GENERIC_VACANCY_ERROR_MSG = (
    "–ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —Å–∞–π—Ç–∞.\n"
    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –≤—Ä—É—á–Ω—É—é."
)

# =========================
# –ü–ï–†–ï–ú–ï–ù–ù–´–ï –ò–ó ENVIRONMENT
# =========================

# –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ User-Agent
DESKTOP_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
MOBILE_USER_AGENT = "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1"

# Headers –¥–ª—è –¥–µ—Å–∫—Ç–æ–ø
BROWSER_HEADERS = {
    "User-Agent": DESKTOP_USER_AGENT,
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
    "Cache-Control": "max-age=0",
}

# Headers –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö
MOBILE_HEADERS = {
    "User-Agent": MOBILE_USER_AGENT,
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ru-RU,ru;q=0.9",
    "Accept-Encoding": "gzip, deflate, br",
}

# =========================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# =========================

def clean_text(raw: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    if not raw:
        return ""
    text = raw.replace("\r\n", "\n").replace("\r", "\n")
    lines = [line.strip() for line in text.split("\n")]
    text = "\n".join(lines)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def extract_text_from_pdf_bytes(data: bytes) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF"""
    try:
        reader = PdfReader(BytesIO(data))
        pages_text = []

        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                pages_text.append(page_text)

        text = "\n\n".join(pages_text)
        text = clean_text(text)

        if not text or len(text) < 50:
            raise ValueError("PDF —Ñ–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Ç–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")

        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ PDF")
        return text

    except ValueError:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}", exc_info=True)
        raise ValueError(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å PDF —Ñ–∞–π–ª. "
            "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –ø–æ–≤—Ä–µ–∂–¥—ë–Ω –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç."
        )

def looks_like_url(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ URL"""
    if not text:
        return False
    text = text.strip()
    URL_REGEX = re.compile(r"^(https?://)?([a-z0-9.-]+\.[a-z]{2,})(/.*)?$", re.IGNORECASE)
    return bool(URL_REGEX.match(text))

def normalize_url(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL"""
    text = text.strip()
    if not text.startswith(("http://", "https://")):
        return "https://" + text
    return text

def html_to_text(html: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML"""
    if not html:
        return ""
    
    try:
        soup = BeautifulSoup(html, "lxml")  # –ò—Å–ø–æ–ª—å–∑—É–µ–º lxml –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in soup(["script", "style", "nav", "footer", "header", "aside", "form", "iframe", "button"]):
            element.decompose()
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –¥–ª—è HH.ru
        if 'hh.ru' in html.lower():
            # –£–¥–∞–ª—è–µ–º –±–ª–æ–∫–∏ —Å –ø–æ—Ö–æ–∂–∏–º–∏ –≤–∞–∫–∞–Ω—Å–∏—è–º–∏, —Ä–µ–∫–ª–∞–º–æ–π –∏ —Ç.–¥.
            for element in soup.find_all(class_=re.compile(r'(vacancy-serp-item|sidebar|related|similar|recommended|bloko-column)')):
                element.decompose()
        
        text = soup.get_text(separator='\n', strip=True)
        text = clean_text(text)
        
        # –£–¥–∞–ª—è–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
        lines = [line for line in text.split('\n') if len(line.strip()) > 5]
        text = '\n'.join(lines)
        
        return text
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
        # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç–æ–π regex fallback
        try:
            text = re.sub(r'<[^>]+>', '\n', html)
            text = re.sub(r'\n{3,}', '\n\n', text)
            return clean_text(text)
        except:
            return ""

def _format_proxy_for_requests(proxy_url: str) -> Optional[dict]:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–∫—Å–∏ –¥–ª—è requests"""
    if not proxy_url:
        return None
    
    proxy = proxy_url.strip()
    
    if proxy.startswith(('http://', 'https://', 'socks5://')):
        return {'http': proxy, 'https': proxy}
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ö–µ–º—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if '@' in proxy:
        return {'http': f"http://{proxy}", 'https': f"http://{proxy}"}
    else:
        return {'http': f"http://{proxy}", 'https': f"http://{proxy}"}

# =========================
# –ú–ï–¢–û–î 1: –ü–†–û–°–¢–û–ô –ó–ê–ü–†–û–° —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏
# =========================

def _try_simple_request(url: str, use_proxy: bool = True, force_mobile: bool = False) -> Tuple[bool, str, Optional[str]]:
    """–ü—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç–æ –∑–∞–π—Ç–∏ –∫–∞–∫ –æ–±—ã—á–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä"""
    
    proxies = None
    if use_proxy and PROXY_URL:
        proxies = _format_proxy_for_requests(PROXY_URL)
    
    try:
        # –í—ã–±–∏—Ä–∞–µ–º headers
        if force_mobile:
            headers = MOBILE_HEADERS.copy()
            # –î–ª—è HH.ru –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º URL –≤ –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é
            if 'hh.ru' in url and not url.startswith(('https://m.hh.ru', 'http://m.hh.ru')):
                url = url.replace('https://hh.ru', 'https://m.hh.ru')
                url = url.replace('http://hh.ru', 'http://m.hh.ru')
                logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é HH: {url}")
        else:
            headers = BROWSER_HEADERS.copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        headers.update({
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'DNT': '1',
        })
        
        session = requests.Session()
        session.headers.update(headers)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É
        time.sleep(random.uniform(1, 2))
        
        response = session.get(
            url, 
            proxies=proxies, 
            timeout=15, 
            allow_redirects=True,
            verify=False  # –ú–æ–∂–µ—Ç –ø–æ–º–æ—á—å —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ SSL
        )
        
        logger.info(f"–°—Ç–∞—Ç—É—Å: {response.status_code}, —Ä–∞–∑–º–µ—Ä: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        html = response.text
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–∞–ø—á—É –∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        html_lower = html.lower()
        has_captcha = any(x in html_lower for x in [
            'captcha', 'cloudflare', 'access denied', 
            'ddos-guard', 'recaptcha', 'hcaptcha',
            '–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ —á—Ç–æ –≤—ã –Ω–µ —Ä–æ–±–æ—Ç'
        ])
        
        if response.status_code == 200 and not has_captcha and len(html) > 800:
            logger.info(f"‚úÖ –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –£–°–ü–ï–®–ï–ù!")
            return True, html, None
        else:
            if has_captcha:
                return False, html, "–ö–∞–ø—á–∞/–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞"
            elif response.status_code == 403:
                return False, html, "403 Forbidden"
            elif response.status_code == 429:
                return False, html, "429 Too Many Requests"
            elif len(html) < 500:
                return False, html, "–ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç"
            else:
                return False, html, f"HTTP {response.status_code}"
                
    except requests.exceptions.SSLError:
        # –ü—Ä–æ–±—É–µ–º –±–µ–∑ SSL –ø—Ä–æ–≤–µ—Ä–∫–∏
        try:
            response = requests.get(url, headers=headers, timeout=15, verify=False)
            if response.status_code == 200 and len(response.text) > 800:
                return True, response.text, None
            return False, response.text, f"HTTP {response.status_code} (SSL bypass)"
        except Exception as e:
            return False, "", f"SSL Error: {e}"
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ—Å—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        return False, "", str(e)

# =========================
# –ú–ï–¢–û–î 2: CLOUDSCRAPER (–æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è HH.ru)
# =========================

def _try_cloudscraper(url: str) -> Tuple[bool, str, Optional[str]]:
    """–ü—Ä–æ–±—É–µ–º Cloudscraper - –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã"""
    if not CLOUDSCRAPER_ENABLED:
        return False, "", "Cloudscraper –æ—Ç–∫–ª—é—á–µ–Ω"
    
    try:
        import cloudscraper
        
        logger.info(f"üîÑ –ü—Ä–æ–±—É–µ–º Cloudscraper –¥–ª—è {url}")
        
        # –°–æ–∑–¥–∞–µ–º scraper —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False
            },
            delay=10
        )
        
        proxies = None
        if PROXY_URL:
            proxies = _format_proxy_for_requests(PROXY_URL)
        
        # –î–ª—è HH.ru –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é
        if 'hh.ru' in url and FORCE_MOBILE_HH and not url.startswith(('https://m.hh.ru', 'http://m.hh.ru')):
            url = url.replace('https://hh.ru', 'https://m.hh.ru')
            url = url.replace('http://hh.ru', 'http://m.hh.ru')
            logger.info(f"Cloudscraper –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é: {url}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–±–∏–ª—å–Ω—ã–µ headers –¥–ª—è HH.ru
        headers = MOBILE_HEADERS.copy() if 'hh.ru' in url else BROWSER_HEADERS.copy()
        
        response = scraper.get(
            url, 
            headers=headers,
            proxies=proxies, 
            timeout=30
        )
        
        logger.info(f"Cloudscraper —Å—Ç–∞—Ç—É—Å: {response.status_code}")
        
        if response.status_code == 200:
            html = response.text
            
            if len(html) > 800 and 'captcha' not in html.lower():
                logger.info(f"‚úÖ Cloudscraper –£–°–ü–ï–®–ï–ù!")
                return True, html, None
            else:
                return False, html, f"–ö–∞–ø—á–∞ –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç ({len(html)} chars)"
        else:
            return False, response.text, f"HTTP {response.status_code}"
            
    except ImportError:
        logger.warning("Cloudscraper –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        return False, "", "Cloudscraper –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
    except Exception as e:
        logger.error(f"‚ùå Cloudscraper –æ—à–∏–±–∫–∞: {e}")
        return False, "", str(e)

# =========================
# –ú–ï–¢–û–î 3: –†–ï–ó–ï–†–í–ù–´–ô –ú–ï–¢–û–î - API —á–µ—Ä–µ–∑ —Å—Ç–æ—Ä–æ–Ω–Ω–∏–π —Å–µ—Ä–≤–∏—Å
# =========================

def _try_scraping_ant(url: str) -> Tuple[bool, str, Optional[str]]:
    """–†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ ScrapingAnt API (–µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω)"""
    api_key = os.environ.get('SCRAPINGANT_API_KEY')
    if not api_key:
        return False, "", "ScrapingAnt API key not set"
    
    try:
        logger.info(f"üîÑ –ü—Ä–æ–±—É–µ–º ScrapingAnt API –¥–ª—è {url}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é –¥–ª—è HH.ru
        if 'hh.ru' in url and not url.startswith(('https://m.hh.ru', 'http://m.hh.ru')):
            url = url.replace('https://hh.ru', 'https://m.hh.ru')
        
        api_url = f"https://api.scrapingant.com/v2/general"
        params = {
            'url': url,
            'x-api-key': api_key,
            'browser': 'false',  # –ë–µ–∑ –±—Ä–∞—É–∑–µ—Ä–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            'proxy_country': 'RU',
            'return_text': 'true'
        }
        
        response = requests.get(api_url, params=params, timeout=30)
        
        if response.status_code == 200:
            data = response.json()
            html = data.get('text', '')
            
            if len(html) > 800:
                logger.info(f"‚úÖ ScrapingAnt –£–°–ü–ï–®–ï–ù!")
                return True, html, None
            else:
                return False, html, f"–ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç ({len(html)} chars)"
        else:
            return False, "", f"API error: {response.status_code}"
            
    except Exception as e:
        logger.error(f"‚ùå ScrapingAnt –æ—à–∏–±–∫–∞: {e}")
        return False, "", str(e)

# =========================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ü–ê–†–°–ò–ù–ì–ê
# =========================

def fetch_url_text_via_proxy(url: str) -> str:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º Cloudscraper
    """
    logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥: {url}")
    logger.info(f"–ù–∞—Å—Ç—Ä–æ–π–∫–∏: Cloudscraper={CLOUDSCRAPER_ENABLED}, IS_RENDER={IS_RENDER}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL
    if not url or not looks_like_url(url):
        raise ValueError("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞")
    
    url = normalize_url(url)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–æ–¥—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–∫—Ä—É–∂–µ–Ω–∏—è
    if IS_RENDER:
        # –ù–∞ Render –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ª–µ–≥–∫–∏–µ –º–µ—Ç–æ–¥—ã
        methods = [
            ("Cloudscraper", lambda: _try_cloudscraper(url)),
            ("–ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å (–º–æ–±–∏–ª—å–Ω—ã–π)", lambda: _try_simple_request(url, force_mobile=True)),
        ]
    else:
        # –õ–æ–∫–∞–ª—å–Ω–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ –º–µ—Ç–æ–¥—ã
        methods = [
            ("Cloudscraper", lambda: _try_cloudscraper(url)),
            ("–ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å (–º–æ–±–∏–ª—å–Ω—ã–π)", lambda: _try_simple_request(url, force_mobile=True)),
            ("–ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å (–¥–µ—Å–∫—Ç–æ–ø)", lambda: _try_simple_request(url, force_mobile=False)),
        ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º API –º–µ—Ç–æ–¥ –µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á
    if os.environ.get('SCRAPINGANT_API_KEY'):
        methods.append(("ScrapingAnt API", lambda: _try_scraping_ant(url)))
    
    logger.info(f"–ë—É–¥–µ–º –ø—Ä–æ–±–æ–≤–∞—Ç—å {len(methods)} –º–µ—Ç–æ–¥–æ–≤")
    
    # –ü—Ä–æ–±—É–µ–º –≤—Å–µ –º–µ—Ç–æ–¥—ã
    last_error = None
    
    for method_name, method_func in methods:
        try:
            logger.info(f"üîÑ –ü—Ä–æ–±—É–µ–º {method_name}...")
            
            success, html, error = method_func()
            
            if success:
                text = html_to_text(html)
                logger.info(f"{method_name}: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞")
                
                if text and len(text) >= MIN_MEANINGFUL_TEXT_LENGTH:
                    logger.info(f"‚úÖ {method_name} –£–°–ü–ï–®–ï–ù!")
                    return text
                else:
                    logger.warning(f"‚ö†Ô∏è {method_name}: –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    continue
            else:
                logger.warning(f"‚ö†Ô∏è {method_name}: {error}")
                last_error = error
                continue
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è {method_name} –≤—ã–∑–≤–∞–ª –∏—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")
            last_error = e
            continue
    
    # –í—Å–µ –º–µ—Ç–æ–¥—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏
    logger.error(f"‚ùå –í—Å–µ –º–µ—Ç–æ–¥—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–ª—è {url}")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    if 'hh.ru' in url:
        error_msg = (
            "HH.ru –∞–∫—Ç–∏–≤–Ω–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã.\n\n"
            "üîß **–ö–∞–∫ —Ä–µ—à–∏—Ç—å:**\n"
            "1. –û—Ç–∫—Ä–æ–π—Ç–µ —Å—Å—ã–ª–∫—É –≤ –±—Ä–∞—É–∑–µ—Ä–µ\n"
            "2. –ù–∞–∂–º–∏—Ç–µ Ctrl+A (–≤—ã–¥–µ–ª–∏—Ç—å –≤—Å–µ)\n"
            "3. –ù–∞–∂–º–∏—Ç–µ Ctrl+C (—Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å)\n"
            "4. –û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç —Å—é–¥–∞\n\n"
            "üìù –ò–ª–∏ –ø—Ä–∏—à–ª–∏—Ç–µ –≤–∞–∫–∞–Ω—Å–∏—é —Ç–µ–∫—Å—Ç–æ–º –≤—Ä—É—á–Ω—É—é"
        )
    elif 'habr.com' in url:
        error_msg = (
            "–î–ª—è –≤–∞–∫–∞–Ω—Å–∏–π —Å Habr –ª—É—á—à–µ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –≤—Ä—É—á–Ω—É—é.\n"
            "–ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ direct —Å—Å—ã–ª–∫—É –Ω–∞ –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏."
        )
    elif last_error and ("403" in str(last_error)):
        error_msg = "–°–∞–π—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –¥–æ—Å—Ç—É–ø (403 Forbidden). –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ç–µ–∫—Å—Ç –≤—Ä—É—á–Ω—É—é."
    elif last_error and ("429" in str(last_error)):
        error_msg = "–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–¥–æ–∂–¥–∏—Ç–µ 5 –º–∏–Ω—É—Ç –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
    else:
        error_msg = GENERIC_VACANCY_ERROR_MSG
    
    raise ValueError(error_msg)

# =========================
# –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –†–ï–ó–Æ–ú–ï (PDF)
# =========================

def parse_resume_from_pdf(pdf_content: bytes) -> str:
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—é–º–µ –∏–∑ PDF"""
    try:
        text = extract_text_from_pdf_bytes(pdf_content)
        if len(text) < 100:
            raise ValueError("–†–µ–∑—é–º–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ –∏–ª–∏ –Ω–µ—á–∏—Ç–∞–µ–º–æ–µ")
        return text
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—é–º–µ: {e}")
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ä–µ–∑—é–º–µ. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ PDF –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç.")

# =========================
# –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –í–ê–ö–ê–ù–°–ò–ò (URL)
# =========================

def parse_vacancy_from_url(url: str) -> str:
    """–ü–∞—Ä—Å–∏–Ω–≥ –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ URL"""
    try:
        text = fetch_url_text_via_proxy(url)
        if len(text) < 200:
            raise ValueError("–¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
        return text
    except ValueError as e:
        # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∫–∞–∫ –µ—Å—Ç—å
        raise e
    except Exception as e:
        logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        raise ValueError(GENERIC_VACANCY_ERROR_MSG)
