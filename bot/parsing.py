import logging
import re
import time
import random
import os
from io import BytesIO
from typing import Optional, Tuple, List
from urllib.parse import urlparse

import requests
from bs4 import BeautifulSoup
from PyPDF2 import PdfReader
from fake_useragent import UserAgent

from config import (
    PROXY_URL,
    MIN_MEANINGFUL_TEXT_LENGTH,
    CLOUDSCRAPER_ENABLED,
    FORCE_MOBILE_HH,
    RETRY_COUNT,
    IS_RENDER,
)

logger = logging.getLogger(__name__)

# =========================
# –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ü–û –°–¢–ê–¢–¨–ï
# =========================
ua = UserAgent(browsers=["chrome", "edge", "firefox"], os=["windows", "linux", "macos"])


# =========================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –¢–ï–ö–°–¢–ê / PDF
# =========================

def clean_text(raw: str) -> str:
    if not raw:
        return ""
    text = raw.replace("\r\n", "\n").replace("\r", "\n")
    lines = [line.strip() for line in text.split("\n")]
    text = "\n".join(lines)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()


def extract_text_from_pdf_bytes(data: bytes) -> str:
    try:
        reader = PdfReader(BytesIO(data))
        pages_text = []

        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                pages_text.append(page_text)

        text = "\n\n".join(pages_text)
        text = clean_text(text)

        if not text or len(text) < 50:
            raise ValueError("PDF —Ñ–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Ç–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")

        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ PDF")
        return text

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}", exc_info=True)
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å PDF —Ñ–∞–π–ª.")


def looks_like_url(text: str) -> bool:
    if not text:
        return False
    text = text.strip()
    url_regex = re.compile(r"^(https?://)?([a-z0-9.-]+\.[a-z]{2,})(/.*)?$", re.IGNORECASE)
    return bool(url_regex.match(text))


def normalize_url(text: str) -> str:
    text = text.strip()
    if not text.startswith(("http://", "https://")):
        return "https://" + text
    return text


def html_to_text(html: str) -> str:
    if not html:
        return ""

    try:
        soup = BeautifulSoup(html, "lxml")

        # –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in soup(
            ["script", "style", "nav", "footer", "header", "aside", "form", "iframe", "button"]
        ):
            element.decompose()

        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è —á–∏—Å—Ç–∫–∞ –¥–ª—è hh.ru
        if "hh.ru" in html.lower():
            for element in soup.find_all(
                class_=re.compile(
                    r"(vacancy-serp-item|sidebar|related|similar|recommended|bloko-column)"
                )
            ):
                element.decompose()

        text = soup.get_text(separator="\n", strip=True)
        text = clean_text(text)

        # –£–±–∏—Ä–∞–µ–º —Å–æ–≤—Å–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
        lines = [line for line in text.split("\n") if len(line.strip()) > 5]
        text = "\n".join(lines)

        return text

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
        return ""


# =========================
# –ü–†–û–ö–°–ò
# =========================

def _format_proxy_for_requests(proxy_url: str) -> Optional[dict]:
    if not proxy_url:
        return None

    proxy = proxy_url.strip()

    if proxy.startswith(("http://", "https://", "socks5://")):
        return {"http": proxy, "https": proxy}

    if "@" in proxy:
        return {"http": f"http://{proxy}", "https": f"http://{proxy}"}
    else:
        return {"http": f"http://{proxy}", "https": f"http://{proxy}"}


# =========================
# –ú–ï–¢–û–î 1: –ü–†–û–°–¢–û–ô –ó–ê–ü–†–û–°
# =========================

def _try_simple_request(
    url: str,
    use_proxy: bool = True,
    force_mobile: bool = False,
) -> Tuple[bool, str, Optional[str]]:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π requests-–∑–∞–ø—Ä–æ—Å —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º User-Agent"""

    proxies = None
    if use_proxy and PROXY_URL:
        proxies = _format_proxy_for_requests(PROXY_URL)

    try:
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π User-Agent
        user_agent = ua.random

        # –î–ª—è HH.ru –º–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è
        if force_mobile and "hh.ru" in url and not url.startswith(("https://m.hh.ru", "http://m.hh.ru")):
            url = url.replace("https://hh.ru", "https://m.hh.ru")
            url = url.replace("http://hh.ru", "http://m.hh.ru")
            user_agent = (
                "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) "
                "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 "
                "Mobile/15E148 Safari/604.1"
            )

        headers = {
            "User-Agent": user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Cache-Control": "max-age=0",
            "DNT": "1",
        }

        session = requests.Session()
        session.headers.update(headers)

        # –ö—É–∫–∏, —á—Ç–æ–±—ã –ø—Ä–∏—Ç–≤–æ—Ä–∏—Ç—å—Å—è –∂–∏–≤—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
        session.cookies.update(
            {
                "accept": "1",
                "force_cookie_consent": "true",
            }
        )

        # –õ—ë–≥–∫–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
        time.sleep(random.uniform(1, 3))

        response = session.get(
            url,
            proxies=proxies,
            timeout=15,
            allow_redirects=True,
            verify=False,
        )

        logger.info(f"–ó–∞–ø—Ä–æ—Å: —Å—Ç–∞—Ç—É—Å {response.status_code}, —Ä–∞–∑–º–µ—Ä {len(response.text)}")

        html = response.text
        html_lower = html.lower()

        has_captcha = any(
            x in html_lower
            for x in [
                "captcha",
                "cloudflare",
                "access denied",
                "ddos-guard",
                "recaptcha",
                "hcaptcha",
                "–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ —á—Ç–æ –≤—ã –Ω–µ —Ä–æ–±–æ—Ç",
            ]
        )

        if response.status_code == 200 and not has_captcha and len(html) > 1000:
            return True, html, None
        else:
            if has_captcha:
                return False, html, "–ö–∞–ø—á–∞/–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞"
            elif response.status_code == 403:
                return False, html, "403 Forbidden"
            elif response.status_code == 429:
                return False, html, "429 Too Many Requests"
            else:
                return False, html, f"HTTP {response.status_code}"

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        return False, "", str(e)


# =========================
# –ú–ï–¢–û–î 2: CLOUDSCRAPER
# =========================

def _try_cloudscraper(url: str) -> Tuple[bool, str, Optional[str]]:
    """Cloudscraper –∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ"""
    if not CLOUDSCRAPER_ENABLED:
        return False, "", "Cloudscraper –æ—Ç–∫–ª—é—á–µ–Ω"

    try:
        import cloudscraper

        logger.info(f"üîÑ Cloudscraper –¥–ª—è {url}")

        scraper = cloudscraper.create_scraper(
            browser={"browser": "chrome", "platform": "windows", "mobile": False},
            delay=10,
        )

        proxies = None
        if PROXY_URL:
            proxies = _format_proxy_for_requests(PROXY_URL)

        user_agent = ua.random

        # –î–ª—è HH.ru –º–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è
        if FORCE_MOBILE_HH and "hh.ru" in url and not url.startswith(("https://m.hh.ru", "http://m.hh.ru")):
            url = url.replace("https://hh.ru", "https://m.hh.ru")
            url = url.replace("http://hh.ru", "http://m.hh.ru")
            user_agent = (
                "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) "
                "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 "
                "Mobile/15E148 Safari/604.1"
            )

        headers = {
            "User-Agent": user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
        }

        response = scraper.get(
            url,
            headers=headers,
            proxies=proxies,
            timeout=30,
        )

        if response.status_code == 200:
            html = response.text

            if len(html) > 1000 and "captcha" not in html.lower():
                logger.info("‚úÖ Cloudscraper –£–°–ü–ï–®–ï–ù!")
                return True, html, None
            else:
                return False, html, f"–ö–∞–ø—á–∞ –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç ({len(html)} chars)"
        else:
            return False, response.text, f"HTTP {response.status_code}"

    except ImportError:
        logger.warning("Cloudscraper –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        return False, "", "Cloudscraper –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
    except Exception as e:
        logger.error(f"‚ùå Cloudscraper –æ—à–∏–±–∫–∞: {e}")
        return False, "", str(e)


# =========================
# –ü–û–ò–°–ö –ë–ò–ù–ê–†–ù–ò–ö–ê CHROME
# =========================

def _detect_chrome_binary() -> Optional[str]:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –±–∏–Ω–∞—Ä–Ω–∏–∫ Chrome:
    - —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è CHROME_BINARY_PATH
    - —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—É—Ç–∏ (–≤ —Ç.—á. Render c dpkg -x)
    """
    env_path = os.getenv("CHROME_BINARY_PATH")
    if env_path and os.path.exists(env_path):
        return env_path

    candidates: List[str] = [
        "/opt/render/project/.render/chrome/opt/google/chrome/google-chrome",
        "/usr/bin/google-chrome",
        "/usr/bin/google-chrome-stable",
        "/usr/bin/chromium",
        "/usr/bin/chromium-browser",
    ]
    for path in candidates:
        if os.path.exists(path):
            return path

    return None


# =========================
# –ú–ï–¢–û–î 3: UNDETECTED CHROMEDRIVER (Selenium + Chrome)
# =========================

def _try_undetected_chromedriver(url: str) -> Tuple[bool, str, Optional[str]]:
    """Undetected ChromeDriver - –∫–ª—é—á–µ–≤–æ–π –º–µ—Ç–æ–¥ Selenium/Chrome"""
    try:
        import undetected_chromedriver as uc

        logger.info(f"3. Undetected ChromeDriver –¥–ª—è {url}")

        options = uc.ChromeOptions()

        # –ë–∏–Ω–∞—Ä–Ω–∏–∫ Chrome
        chrome_binary = _detect_chrome_binary()
        if chrome_binary:
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º Chrome binary: {chrome_binary}")
            options.binary_location = chrome_binary

        headless_env = os.getenv("SELENIUM_HEADLESS", "true").lower() == "true"
        if headless_env or IS_RENDER:
            options.add_argument("--headless=new")

        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--disable-features=IsolateOrigins,site-per-process")
        options.add_argument("--disable-logging")
        options.add_argument("--log-level=3")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-popup-blocking")
        options.add_argument("--disable-notifications")

        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π User-Agent
        user_agent = ua.random
        options.add_argument(f"--user-agent={user_agent}")

        # –ü—Ä–æ–∫—Å–∏ –¥–ª—è –±—Ä–∞—É–∑–µ—Ä–∞
        if PROXY_URL:
            proxy = PROXY_URL.strip()
            if proxy.startswith("http://"):
                proxy = proxy[7:]
            elif proxy.startswith("https://"):
                proxy = proxy[8:]
            if "@" in proxy:
                proxy = proxy.split("@")[-1]
            options.add_argument(f"--proxy-server={proxy}")

        try:
            driver = uc.Chrome(
                options=options,
                version_main=131,  # –ø–æ–¥ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π Chrome
                suppress_welcome=True,
            )

            try:
                # Stealth
                driver.execute_script(
                    "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
                )
                driver.execute_script(
                    "Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})"
                )
                driver.execute_script(
                    "Object.defineProperty(navigator, 'languages', {get: () => ['ru-RU', 'ru', 'en-US', 'en']})"
                )

                # –ú–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è HH
                if FORCE_MOBILE_HH and "hh.ru" in url and not url.startswith(
                    ("https://m.hh.ru", "http://m.hh.ru")
                ):
                    url = url.replace("https://hh.ru", "https://m.hh.ru")
                    url = url.replace("http://hh.ru", "http://m.hh.ru")

                driver.get(url)

                # –ò–º–∏—Ç–∞—Ü–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è
                time.sleep(random.uniform(2, 4))

                scroll_height = driver.execute_script("return document.body.scrollHeight")
                scroll_steps = random.randint(3, 6)
                for i in range(scroll_steps):
                    scroll_pos = int((i + 1) * (scroll_height / scroll_steps))
                    driver.execute_script(f"window.scrollTo(0, {scroll_pos});")
                    time.sleep(random.uniform(0.2, 0.5))

                time.sleep(random.uniform(1, 2))

                html = driver.page_source

                if len(html) < 1000:
                    return False, html, "–ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç"

                if any(
                    x in html.lower()
                    for x in ["captcha", "cloudflare", "access denied"]
                ):
                    return False, html, "–ö–∞–ø—á–∞/–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞"

                logger.info("‚úÖ Undetected ChromeDriver –£–°–ü–ï–®–ï–ù!")
                return True, html, None

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ ChromeDriver: {e}")
                return False, "", str(e)
            finally:
                try:
                    driver.quit()
                except Exception:
                    pass

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥—Ä–∞–π–≤–µ—Ä–∞: {e}")
            return False, "", str(e)

    except ImportError:
        logger.warning("Undetected ChromeDriver –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        return False, "", "Undetected ChromeDriver –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
    except Exception as e:
        logger.error(f"‚ùå Undetected ChromeDriver –æ—à–∏–±–∫–∞: {e}")
        return False, "", str(e)


# =========================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ü–ê–†–°–ò–ù–ì–ê
# =========================

def fetch_url_text_via_proxy(url: str) -> str:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞:
    1. Cloudscraper
    2. Undetected ChromeDriver
    3. –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å (–º–æ–±–∏–ª—å–Ω—ã–π)
    4. –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å (–¥–µ—Å–∫—Ç–æ–ø)
    –° —É—á—ë—Ç–æ–º RETRY_COUNT.
    """
    logger.info(f"üöÄ –ü–∞—Ä—Å–∏–Ω–≥ –≤–∞–∫–∞–Ω—Å–∏–∏: {url}")

    if not url or not looks_like_url(url):
        raise ValueError("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞")

    url = normalize_url(url)

    methods = [
        ("Cloudscraper", lambda: _try_cloudscraper(url)),
        ("Undetected ChromeDriver", lambda: _try_undetected_chromedriver(url)),
        ("–ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å (–º–æ–±–∏–ª—å–Ω—ã–π)", lambda: _try_simple_request(url, force_mobile=True)),
        ("–ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å (–¥–µ—Å–∫—Ç–æ–ø)", lambda: _try_simple_request(url, force_mobile=False)),
    ]

    logger.info(f"–ú–µ—Ç–æ–¥—ã –ø–∞—Ä—Å–∏–Ω–≥–∞: {[m[0] for m in methods]}")

    last_error: Optional[object] = None
    attempts = max(1, RETRY_COUNT)

    for attempt in range(1, attempts + 1):
        logger.info(f"üîÅ –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{attempts}")
        for method_name, method_func in methods:
            try:
                logger.info(f"üîÑ –ü—Ä–æ–±—É–µ–º {method_name}...")
                success, html, error = method_func()

                if success:
                    text = html_to_text(html)

                    if text and len(text) >= MIN_MEANINGFUL_TEXT_LENGTH:
                        logger.info(
                            f"‚úÖ {method_name} –£–°–ü–ï–®–ï–ù! ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞)"
                        )
                        return text
                    else:
                        logger.warning(
                            f"‚ö†Ô∏è {method_name}: –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)"
                        )
                        continue
                else:
                    logger.warning(f"‚ö†Ô∏è {method_name}: {error}")
                    last_error = error
                    continue

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è {method_name}: –∏—Å–∫–ª—é—á–µ–Ω–∏–µ {e}")
                last_error = e
                continue

        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Ä–µ—Ç—Ä–∞—è–º–∏
        time.sleep(random.uniform(1, 2))

    # –í—Å–µ –º–µ—Ç–æ–¥—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏ ‚Äî –û–î–ù–û –∞–∫–∫—É—Ä–∞—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    error_msg = (
        "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ —Å—Å—ã–ª–∫–µ.\n\n"
        "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞:\n"
        "1. –û—Ç–∫—Ä–æ–π—Ç–µ —Å—Å—ã–ª–∫—É –≤ –±—Ä–∞—É–∑–µ—Ä–µ\n"
        "2. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ç–µ–∫—Å—Ç\n"
        "3. –û—Ç–ø—Ä–∞–≤—å—Ç–µ –µ–≥–æ —Å—é–¥–∞"
    )

    raise ValueError(error_msg)


# =========================
# –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ë–û–¢–ê (PDF + URL)
# =========================

def parse_resume_from_pdf(pdf_content: bytes) -> str:
    try:
        text = extract_text_from_pdf_bytes(pdf_content)
        if len(text) < 100:
            raise ValueError("–†–µ–∑—é–º–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ")
        return text
    except Exception:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ä–µ–∑—é–º–µ.")


def parse_vacancy_from_url(url: str) -> str:
    try:
        text = fetch_url_text_via_proxy(url)
        if len(text) < 200:
            raise ValueError("–¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
        return text
    except ValueError as e:
        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        raise e
    except Exception:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–∫–∞–Ω—Å–∏—é.")
