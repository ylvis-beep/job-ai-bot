import logging
import re
import time
import random
import os
from io import BytesIO
from typing import Optional, Dict, Any
from urllib.parse import urlparse

import requests
from bs4 import BeautifulSoup
from PyPDF2 import PdfReader

from config import (
    PROXY_URL, 
    MIN_MEANINGFUL_TEXT_LENGTH,
    SELENIUM_ENABLED,
    SELENIUM_TIMEOUT,
    SELENIUM_HEADLESS
)

logger = logging.getLogger(__name__)

# –ï–¥–∏–Ω–æ–µ "—á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ" —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
GENERIC_VACANCY_ERROR_MSG = (
    "–ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —Å–∞–π—Ç–∞.\n"
    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –≤—Ä—É—á–Ω—É—é."
)

# =========================
# –ù–ê–°–¢–†–û–ô–ö–ò –ò–ó –°–¢–ê–¢–¨–ò
# =========================

# –í–∫–ª—é—á–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤
CLOUDSCRAPER_ENABLED = os.getenv("CLOUDSCRAPER_ENABLED", "true").lower() == "true"
PLAYWRIGHT_ENABLED = os.getenv("PLAYWRIGHT_ENABLED", "false").lower() == "true"  # false –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º –Ω–∞ Render

# –ü–æ–ª–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –±—Ä–∞—É–∑–µ—Ä–∞
FULL_BROWSER_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
    "Cache-Control": "max-age=0",
    "TE": "trailers",
}

# =========================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# =========================

def clean_text(raw: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    if not raw:
        return ""
    text = raw.replace("\r\n", "\n").replace("\r", "\n")
    lines = [line.strip() for line in text.split("\n")]
    text = "\n".join(lines)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def extract_text_from_pdf_bytes(data: bytes) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF"""
    try:
        reader = PdfReader(BytesIO(data))
        pages_text = []

        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                pages_text.append(page_text)

        text = "\n\n".join(pages_text)
        text = clean_text(text)

        if not text or len(text) < 50:
            raise ValueError("PDF —Ñ–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Ç–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")

        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ PDF")
        return text

    except ValueError:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}", exc_info=True)
        raise ValueError(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å PDF —Ñ–∞–π–ª. "
            "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –ø–æ–≤—Ä–µ–∂–¥—ë–Ω –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç."
        )

# =========================
# –õ–û–ì–ò–ö–ê –†–ê–ë–û–¢–´ –°–û –°–°–´–õ–ö–ê–ú–ò
# =========================

URL_REGEX = re.compile(
    r"^(https?://)?([a-z0-9.-]+\.[a-z]{2,})(/.*)?$",
    re.IGNORECASE,
)

def looks_like_url(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, –ø–æ—Ö–æ–∂–∞ –ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–∞ URL"""
    if not text:
        return False
    text = text.strip()
    return bool(URL_REGEX.match(text))

def normalize_url(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL"""
    text = text.strip()
    if not text.startswith(("http://", "https://")):
        return "https://" + text
    return text

def html_to_text(html: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML"""
    try:
        soup = BeautifulSoup(html, "html.parser")
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in soup(["script", "style", "nav", "footer", "header", "aside", "form"]):
            element.decompose()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç
        text = soup.get_text(separator='\n', strip=True)
        
        # –û—á–∏—â–∞–µ–º
        text = clean_text(text)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        lines = [line for line in text.split('\n') if line.strip()]
        text = '\n'.join(lines)
        
        return text
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML: {e}", exc_info=True)
        return ""

# =========================
# –ü–†–û–í–ï–†–ö–ò –ò –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# =========================

def _is_blocked_page(html: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∞–ø—á—É/–±–ª–æ–∫–∏—Ä–æ–≤–∫—É"""
    if not html or len(html) < 100:
        logger.warning("–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π HTML, –≤–æ–∑–º–æ–∂–Ω–æ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
        return True
    
    html_lower = html.lower()
    
    block_indicators = [
        "captcha",
        "cloudflare",
        "access denied",
        "–¥–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω",
        "403 forbidden",
        "are you human",
        "–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ —á—Ç–æ –≤—ã –Ω–µ —Ä–æ–±–æ—Ç",
        "security check",
        "ddos-guard",
        "recaptcha",
    ]
    
    for indicator in block_indicators:
        if indicator in html_lower:
            logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏: {indicator}")
            return True
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞–ø—á–∏
    if 'captcha' in html_lower and ('input' in html_lower or 'form' in html_lower):
        return True
    
    return False

def _normalize_proxy_url(raw: str) -> Optional[str]:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∫—Å–∏ –¥–ª—è requests"""
    if not raw:
        return None
    
    raw = raw.strip()
    logger.info(f"–ò—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–∫—Å–∏: {raw[:50]}...")
    
    # –£–∂–µ –µ—Å—Ç—å —Å—Ö–µ–º–∞
    if re.match(r"^[a-zA-Z0-9+.-]+://", raw):
        logger.info(f"–ü—Ä–æ–∫—Å–∏ —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω: {raw[:50]}...")
        return raw
    
    # –§–æ—Ä–º–∞—Ç—ã: user:pass@host:port –∏–ª–∏ host:port@user:pass
    if "@" in raw:
        parts = raw.split("@")
        if len(parts) == 2:
            left, right = parts
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–¥–µ –ª–æ–≥–∏–Ω:–ø–∞—Ä–æ–ª—å, –∞ –≥–¥–µ —Ö–æ—Å—Ç:–ø–æ—Ä—Ç
            if ":" in left and ":" in right:
                # –û–±–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç –¥–≤–æ–µ—Ç–æ—á–∏–µ, –Ω—É–∂–Ω–æ –ø–æ–Ω—è—Ç—å —á—Ç–æ –µ—Å—Ç—å —á—Ç–æ
                # –û–±—ã—á–Ω–æ —Ö–æ—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ—á–∫—É
                if "." in left and not "." in right:
                    # left = host:port, right = user:pass
                    host_port, credentials = left, right
                else:
                    # left = user:pass, right = host:port
                    credentials, host_port = left, right
                
                logger.info(f"–§–æ—Ä–º–∞—Ç: {credentials}@{host_port}")
                return f"http://{credentials}@{host_port}"
    
    # –ü—Ä–æ—Å—Ç–æ–π host:port
    logger.info(f"–ü—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç: {raw}")
    return f"http://{raw}"

# =========================
# CLOUDSCRAPER (–û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î)
# =========================

def parse_with_cloudscraper(url: str, proxy_url: Optional[str] = None) -> str:
    """–û–±—Ö–æ–¥ Cloudflare —á–µ—Ä–µ–∑ cloudscraper"""
    try:
        import cloudscraper
        
        logger.info(f"‚òÅÔ∏è Cloudscraper: –Ω–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {url}")
        
        # –°–æ–∑–¥–∞–µ–º scraper
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False
            }
        )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–æ–∫—Å–∏
        proxies = None
        if proxy_url:
            normalized_proxy = _normalize_proxy_url(proxy_url)
            if normalized_proxy:
                proxies = {
                    'http': normalized_proxy,
                    'https': normalized_proxy
                }
                logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–∫—Å–∏: {normalized_proxy[:50]}...")
        
        # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å
        logger.info(f"–î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ {url}")
        start_time = time.time()
        
        response = scraper.get(
            url, 
            headers=FULL_BROWSER_HEADERS,
            proxies=proxies,
            timeout=30
        )
        
        elapsed = time.time() - start_time
        logger.info(f"Cloudscraper –æ—Ç–≤–µ—Ç–∏–ª –∑–∞ {elapsed:.2f} —Å–µ–∫, —Å—Ç–∞—Ç—É—Å: {response.status_code}")
        
        if response.status_code != 200:
            logger.warning(f"Cloudscraper: —Å—Ç–∞—Ç—É—Å {response.status_code}")
            if response.status_code == 403:
                raise ValueError("–°–∞–π—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –¥–æ—Å—Ç—É–ø (403)")
            elif response.status_code == 429:
                raise ValueError("–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤ (429)")
            else:
                response.raise_for_status()
        
        html = response.text
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(html)} —Å–∏–º–≤–æ–ª–æ–≤ HTML")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
        if _is_blocked_page(html):
            logger.warning("Cloudscraper: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
            raise ValueError("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –∏–ª–∏ –∫–∞–ø—á–∞")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ HTML –Ω–µ –ø—É—Å—Ç–æ–π
        if len(html) < 500:
            logger.warning(f"Cloudscraper: —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π HTML ({len(html)} —Å–∏–º–≤–æ–ª–æ–≤)")
            # –ù–æ –Ω–µ –ø–∞–¥–∞–µ–º —Å—Ä–∞–∑—É, –º–æ–∂–µ—Ç –±—ã—Ç—å –º–∞–ª–µ–Ω—å–∫–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        
        logger.info(f"‚úÖ Cloudscraper —É—Å–ø–µ—à–µ–Ω –¥–ª—è {url}")
        return html
        
    except ImportError as e:
        logger.error(f"Cloudscraper –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
        raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install cloudscraper")
    except Exception as e:
        logger.error(f"‚ùå Cloudscraper –æ—à–∏–±–∫–∞ –¥–ª—è {url}: {str(e)}", exc_info=True)
        raise ValueError(f"Cloudscraper –Ω–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {str(e)}")

# =========================
# UNDETECTED CHROMEDRIVER
# =========================

def parse_with_undetected_chromedriver(url: str, proxy_url: Optional[str] = None) -> str:
    """Undetected ChromeDriver –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–∞–π—Ç–æ–≤"""
    try:
        import undetected_chromedriver as uc
        
        logger.info(f"üõ°Ô∏è Undetected ChromeDriver: –Ω–∞—á–∏–Ω–∞–µ–º {url}")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        options = uc.ChromeOptions()
        
        if SELENIUM_HEADLESS:
            options.add_argument('--headless=new')
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º headless —Ä–µ–∂–∏–º")
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è Render
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-blink-features=AutomationControlled')
        
        # User-Agent
        options.add_argument(f'--user-agent={FULL_BROWSER_HEADERS["User-Agent"]}')
        
        # –ü—Ä–æ–∫—Å–∏
        if proxy_url:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–∫—Å–∏ –¥–ª—è Chrome
            proxy_for_chrome = _format_proxy_for_chrome(proxy_url)
            options.add_argument(f'--proxy-server={proxy_for_chrome}')
            logger.info(f"Undetected —Å –ø—Ä–æ–∫—Å–∏: {proxy_for_chrome}")
        
        # –°–æ–∑–¥–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä
        logger.info("–°–æ–∑–¥–∞–µ–º Undetected ChromeDriver...")
        driver = uc.Chrome(
            options=options,
            version_main=120,  # –í–µ—Ä—Å–∏—è Chrome
            suppress_welcome=True
        )
        
        try:
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            driver.execute_cdp_cmd('Network.setUserAgentOverride', {
                "userAgent": FULL_BROWSER_HEADERS["User-Agent"]
            })
            
            # –°–∫—Ä—ã–≤–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é
            driver.execute_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3, 4, 5]
                });
            """)
            
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            logger.info(f"–û—Ç–∫—Ä—ã–≤–∞–µ–º {url}")
            driver.get(url)
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏
            wait_time = random.uniform(3, 6)
            logger.info(f"–ñ–¥–µ–º {wait_time:.1f} —Å–µ–∫—É–Ω–¥...")
            time.sleep(wait_time)
            
            # –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            logger.info("–ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.3);")
            time.sleep(random.uniform(0.5, 1.5))
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.7);")
            time.sleep(random.uniform(0.5, 1.5))
            
            # –ü–æ–ª—É—á–∞–µ–º HTML
            html = driver.page_source
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            if _is_blocked_page(html):
                logger.warning("Undetected ChromeDriver: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
                raise ValueError("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
            
            logger.info(f"‚úÖ Undetected ChromeDriver —É—Å–ø–µ—à–µ–Ω –¥–ª—è {url}")
            return html
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ Undetected ChromeDriver: {e}")
            raise
        finally:
            try:
                driver.quit()
                logger.info("Undetected ChromeDriver –∑–∞–∫—Ä—ã—Ç")
            except:
                pass
                
    except ImportError as e:
        logger.error(f"Undetected ChromeDriver –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
        raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install undetected-chromedriver")
    except Exception as e:
        logger.error(f"‚ùå Undetected ChromeDriver –æ—à–∏–±–∫–∞: {str(e)}", exc_info=True)
        raise ValueError(f"Undetected ChromeDriver –Ω–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {str(e)}")

def _format_proxy_for_chrome(proxy_url: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–∫—Å–∏ –¥–ª—è Chrome"""
    # –£–±–∏—Ä–∞–µ–º —Å—Ö–µ–º—É
    if proxy_url.startswith('http://'):
        proxy_url = proxy_url[7:]
    elif proxy_url.startswith('https://'):
        proxy_url = proxy_url[8:]
    
    # –£–±–∏—Ä–∞–µ–º –ª–æ–≥–∏–Ω:–ø–∞—Ä–æ–ª—å –µ—Å–ª–∏ –µ—Å—Ç—å (Chrome –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ö)
    if '@' in proxy_url:
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ host:port
        proxy_url = proxy_url.split('@')[1]
    
    return proxy_url

# =========================
# –û–°–ù–û–í–ù–û–ô REQUESTS –ü–ê–†–°–ï–† (FALLBACK)
# =========================

def fetch_html_via_requests(url: str, proxy_url: Optional[str] = None) -> str:
    """–ó–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ requests —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    
    proxies = None
    if proxy_url:
        normalized_proxy = _normalize_proxy_url(proxy_url)
        if normalized_proxy:
            proxies = {"http": normalized_proxy, "https": normalized_proxy}
            logger.info(f"Requests —Å –ø—Ä–æ–∫—Å–∏: {normalized_proxy[:50]}...")
        else:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–æ–∫—Å–∏ URL")
    
    try:
        logger.info(f"üåê Requests: –ø–∞—Ä—Å–∏–º {url}")
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é
        session = requests.Session()
        session.headers.update(FULL_BROWSER_HEADERS)
        
        # –î–æ–±–∞–≤–ª—è–µ–º cookies –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –≤–∏–∑–∏—Ç–∞
        try:
            # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è cookies
            domain = urlparse(url).netloc
            if domain:
                session.get(f"https://{domain}", timeout=5, allow_redirects=True)
                logger.info(f"–ü–æ–ª—É—á–µ–Ω—ã cookies –¥–ª—è {domain}")
        except:
            pass  # –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å
        start_time = time.time()
        response = session.get(url, proxies=proxies, timeout=25, allow_redirects=True)
        elapsed = time.time() - start_time
        
        logger.info(f"Requests –æ—Ç–≤–µ—Ç–∏–ª –∑–∞ {elapsed:.2f} —Å–µ–∫, —Å—Ç–∞—Ç—É—Å: {response.status_code}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å
        if response.status_code != 200:
            logger.warning(f"Requests: HTTP {response.status_code}")
            
            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            html = response.text
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(html)} —Å–∏–º–≤–æ–ª–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ç—É—Å–µ {response.status_code}")
            
            # –ù–æ –≤—Å–µ —Ä–∞–≤–Ω–æ —Å—á–∏—Ç–∞–µ–º –æ—à–∏–±–∫–æ–π
            raise requests.exceptions.HTTPError(f"HTTP {response.status_code}")
        
        html = response.text
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(html)} —Å–∏–º–≤–æ–ª–æ–≤ HTML")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
        if _is_blocked_page(html):
            logger.warning("Requests: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
            raise ValueError("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –∏–ª–∏ –∫–∞–ø—á–∞")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
        if len(html) < 300:  # –£–º–µ–Ω—å—à–∏–ª –ø–æ—Ä–æ–≥ –¥–ª—è —Ç–µ—Å—Ç–∞
            logger.warning(f"Requests: –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç ({len(html)} —Å–∏–º–≤–æ–ª–æ–≤)")
            # –ù–æ –Ω–µ –ø–∞–¥–∞–µ–º —Å—Ä–∞–∑—É
        
        logger.info(f"‚úÖ Requests —É—Å–ø–µ—à–µ–Ω –¥–ª—è {url}")
        return html
        
    except requests.exceptions.Timeout:
        logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}")
        raise ValueError("–°–∞–π—Ç –Ω–µ –æ—Ç–≤–µ–∂–∞–µ—Ç (—Ç–∞–π–º–∞—É—Ç)")
    except requests.exceptions.ProxyError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–∫—Å–∏: {e}")
        raise ValueError("–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏")
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ {url}: {e}")
        raise ValueError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {str(e)}")
    except Exception as e:
        logger.error(f"‚ùå –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ requests: {e}", exc_info=True)
        raise ValueError(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")

# =========================
# –£–õ–£–ß–®–ï–ù–ù–´–ô –£–ú–ù–´–ô –ü–ê–†–°–ï–†
# =========================

def fetch_url_text_via_proxy(url: str) -> str:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –æ—Ç–ª–∞–¥–∫–æ–π –∏ –Ω–∞–¥–µ–∂–Ω—ã–º–∏ fallback'–∞–º–∏
    """
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL
    if not url or not looks_like_url(url):
        logger.error(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL: {url}")
        raise ValueError("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Å—ã–ª–∫–∞")
    
    logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥: {url}")
    logger.info(f"PROXY_URL –¥–æ—Å—Ç—É–ø–µ–Ω: {'–î–∞' if PROXY_URL else '–ù–µ—Ç'}")
    logger.info(f"CLOUDSCRAPER_ENABLED: {CLOUDSCRAPER_ENABLED}")
    logger.info(f"SELENIUM_ENABLED: {SELENIUM_ENABLED}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π Selenium –∫–æ–¥
    def parse_with_selenium_existing(url: str, proxy_url: Optional[str] = None) -> str:
        """–í–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π Selenium –∫–æ–¥"""
        if not SELENIUM_ENABLED:
            raise ValueError("Selenium –æ—Ç–∫–ª—é—á–µ–Ω")
        
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.chrome.service import Service
            from webdriver_manager.chrome import ChromeDriverManager
            
            logger.info(f"ü§ñ Selenium: –Ω–∞—á–∏–Ω–∞–µ–º {url}")
            
            options = Options()
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            
            if SELENIUM_HEADLESS:
                options.add_argument('--headless=new')
                logger.info("Selenium –≤ headless —Ä–µ–∂–∏–º–µ")
            
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            options.add_argument(f'--user-agent={FULL_BROWSER_HEADERS["User-Agent"]}')
            
            if proxy_url:
                proxy_for_selenium = proxy_url
                if proxy_for_selenium.startswith('http://'):
                    proxy_for_selenium = proxy_for_selenium[7:]
                if '@' in proxy_for_selenium:
                    proxy_for_selenium = proxy_for_selenium.split('@')[1]
                options.add_argument(f'--proxy-server={proxy_for_selenium}')
                logger.info(f"Selenium —Å –ø—Ä–æ–∫—Å–∏: {proxy_for_selenium}")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Render
            os.environ['WDM_LOG_LEVEL'] = '0'
            os.environ['WDM_LOCAL'] = '1'
            
            logger.info("–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º ChromeDriver...")
            service = Service(
                ChromeDriverManager(
                    cache_valid_range=30,
                    path="/tmp/chromedriver"
                ).install()
            )
            
            logger.info("–°–æ–∑–¥–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä...")
            driver = webdriver.Chrome(service=service, options=options)
            
            # –°–∫—Ä—ã–≤–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é
            driver.execute_script(
                "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
            )
            
            logger.info(f"–û—Ç–∫—Ä—ã–≤–∞–µ–º {url}")
            driver.get(url)
            
            # –û–∂–∏–¥–∞–Ω–∏–µ
            wait_time = random.uniform(3, 6)
            logger.info(f"–ñ–¥–µ–º {wait_time:.1f} —Å–µ–∫—É–Ω–¥...")
            time.sleep(wait_time)
            
            # –ü—Ä–æ–∫—Ä—É—Ç–∫–∞
            logger.info("–ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.5);")
            time.sleep(random.uniform(1, 2))
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(1, 2))
            
            # –ü–æ–ª—É—á–∞–µ–º HTML
            html = driver.page_source
            logger.info(f"Selenium –ø–æ–ª—É—á–∏–ª {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä
            driver.quit()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            if _is_blocked_page(html):
                logger.warning("Selenium: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
                raise ValueError("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
            
            logger.info(f"‚úÖ Selenium —É—Å–ø–µ—à–µ–Ω –¥–ª—è {url}")
            return html
            
        except Exception as e:
            logger.error(f"‚ùå Selenium –æ—à–∏–±–∫–∞: {str(e)}", exc_info=True)
            raise ValueError(f"Selenium –Ω–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {str(e)}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –º–µ—Ç–æ–¥–æ–≤
    methods_to_try = []
    
    # 1. Cloudscraper (—Å–∞–º—ã–π –ª–µ–≥–∫–∏–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π)
    if CLOUDSCRAPER_ENABLED:
        methods_to_try.append(("Cloudscraper", lambda: parse_with_cloudscraper(url, PROXY_URL)))
        logger.info("–î–æ–±–∞–≤–ª–µ–Ω Cloudscraper –≤ –º–µ—Ç–æ–¥—ã")
    
    # 2. Undetected ChromeDriver (–ª—É—á—à–∏–π –¥–ª—è –∫–∞–ø—á–∏)
    methods_to_try.append(("Undetected ChromeDriver", lambda: parse_with_undetected_chromedriver(url, PROXY_URL)))
    logger.info("–î–æ–±–∞–≤–ª–µ–Ω Undetected ChromeDriver –≤ –º–µ—Ç–æ–¥—ã")
    
    # 3. Selenium (–≤–∞—à —Ç–µ–∫—É—â–∏–π)
    if SELENIUM_ENABLED:
        methods_to_try.append(("Selenium", lambda: parse_with_selenium_existing(url, PROXY_URL)))
        logger.info("–î–æ–±–∞–≤–ª–µ–Ω Selenium –≤ –º–µ—Ç–æ–¥—ã")
    
    # 4. Requests —Å –ø—Ä–æ–∫—Å–∏
    if PROXY_URL:
        methods_to_try.append(("Requests —Å –ø—Ä–æ–∫—Å–∏", lambda: fetch_html_via_requests(url, PROXY_URL)))
        logger.info("–î–æ–±–∞–≤–ª–µ–Ω Requests —Å –ø—Ä–æ–∫—Å–∏ –≤ –º–µ—Ç–æ–¥—ã")
    
    # 5. Requests –±–µ–∑ –ø—Ä–æ–∫—Å–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å)
    methods_to_try.append(("Requests –±–µ–∑ –ø—Ä–æ–∫—Å–∏", lambda: fetch_html_via_requests(url, None)))
    logger.info("–î–æ–±–∞–≤–ª–µ–Ω Requests –±–µ–∑ –ø—Ä–æ–∫—Å–∏ –≤ –º–µ—Ç–æ–¥—ã")
    
    logger.info(f"–í—Å–µ–≥–æ –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞: {len(methods_to_try)}")
    
    # –ü—Ä–æ–±—É–µ–º –≤—Å–µ –º–µ—Ç–æ–¥—ã
    last_error = None
    
    for method_name, parser_func in methods_to_try:
        try:
            logger.info(f"üîÑ –ü—Ä–æ–±—É–µ–º {method_name} –¥–ª—è {url}")
            
            # –ü–∞—Ä—Å–∏–º HTML
            html = parser_func()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text = html_to_text(html)
            logger.info(f"{method_name}: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞
            if text and len(text) >= MIN_MEANINGFUL_TEXT_LENGTH:
                logger.info(f"‚úÖ {method_name} —É—Å–ø–µ—à–µ–Ω! –¢–µ–∫—Å—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                return text
            else:
                logger.warning(f"‚ö†Ô∏è {method_name}: –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ ({len(text) if text else 0} —Å–∏–º–≤–æ–ª–æ–≤)")
                # –ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π –º–µ—Ç–æ–¥
                continue
                
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è {method_name} –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
            continue
        except ValueError as e:
            logger.warning(f"‚ö†Ô∏è {method_name} –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
            last_error = e
            continue
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è {method_name} –≤—ã–∑–≤–∞–ª –∏—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")
            last_error = e
            continue
    
    # –í—Å–µ –º–µ—Ç–æ–¥—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏
    logger.error(f"‚ùå –í—Å–µ –º–µ—Ç–æ–¥—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–ª—è {url}")
    logger.error(f"–ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}")
    
    # –î–∞–µ–º –±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    if last_error and "—Ç–∞–π–º–∞—É—Ç" in str(last_error).lower():
        error_msg = "–°–∞–π—Ç –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –¥—Ä—É–≥—É—é —Å—Å—ã–ª–∫—É."
    elif last_error and ("403" in str(last_error) or "–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª" in str(last_error)):
        error_msg = "–°–∞–π—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –¥–æ—Å—Ç—É–ø. –í–æ–∑–º–æ–∂–Ω–æ, —Ç—Ä–µ–±—É–µ—Ç—Å—è VPN –∏–ª–∏ –¥—Ä—É–≥–æ–π –±—Ä–∞—É–∑–µ—Ä."
    elif last_error and "–ø—Ä–æ–∫—Å–∏" in str(last_error).lower():
        error_msg = "–ü—Ä–æ–±–ª–µ–º–∞ —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥—É—é —Å—Å—ã–ª–∫—É."
    else:
        error_msg = GENERIC_VACANCY_ERROR_MSG
    
    raise ValueError(error_msg)
