import logging
import re
import time
import random
import os
from io import BytesIO
from typing import Optional, Dict, Any  # ‚Üê –î–û–ë–ê–í–¨ –≠–¢–£ –°–¢–†–û–ö–£!

import requests
from bs4 import BeautifulSoup
from PyPDF2 import PdfReader

from config import (
    PROXY_URL, 
    MIN_MEANINGFUL_TEXT_LENGTH,
    SELENIUM_ENABLED,
    SELENIUM_TIMEOUT,
    SELENIUM_HEADLESS
)

logger = logging.getLogger(__name__)

# –ï–¥–∏–Ω–æ–µ "—á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ" —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –∫–æ–≥–¥–∞ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–∫–∞–Ω—Å–∏—é –ø–æ —Å—Å—ã–ª–∫–µ
GENERIC_VACANCY_ERROR_MSG = (
    "–ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —Å–∞–π—Ç–∞.\n"
    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –≤—Ä—É—á–Ω—É—é."
)

# =========================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò–ó –°–¢–ê–¢–¨–ò (–î–û–ë–ê–í–ò–¢–¨ –í Environment –Ω–∞ Render)
# =========================
CLOUDSCRAPER_ENABLED = os.getenv("CLOUDSCRAPER_ENABLED", "false").lower() == "true"
PLAYWRIGHT_ENABLED = os.getenv("PLAYWRIGHT_ENABLED", "false").lower() == "true"

# –ü–æ–ª–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ
FULL_BROWSER_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
    "Cache-Control": "max-age=0",
    "TE": "trailers",
}

# =========================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –¢–ï–ö–°–¢–ê
# =========================

def clean_text(raw: str) -> str:
    if not raw:
        return ""
    text = raw.replace("\r\n", "\n").replace("\r", "\n")
    lines = [line.strip() for line in text.split("\n")]
    text = "\n".join(lines)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

# =========================
# PDF –ü–ê–†–°–ï–†
# =========================

def extract_text_from_pdf_bytes(data: bytes) -> str:
    try:
        reader = PdfReader(BytesIO(data))
        pages_text = []

        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                pages_text.append(page_text)

        text = "\n\n".join(pages_text)
        text = clean_text(text)

        if not text or len(text) < 50:
            raise ValueError("PDF —Ñ–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Ç–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")

        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ PDF")
        return text

    except ValueError:
        # —É–∂–µ "—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è" –æ—à–∏–±–∫–∞
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}", exc_info=True)
        raise ValueError(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å PDF —Ñ–∞–π–ª. "
            "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –ø–æ–≤—Ä–µ–∂–¥—ë–Ω –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç."
        )

# =========================
# –õ–û–ì–ò–ö–ê –†–ê–ë–û–¢–´ –°–û –°–°–´–õ–ö–ê–ú–ò
# =========================

URL_REGEX = re.compile(
    r"^(https?://)?([a-z0-9.-]+\.[a-z]{2,})(/.*)?$",
    re.IGNORECASE,
)

def looks_like_url(text: str) -> bool:
    if not text:
        return False
    text = text.strip()
    return bool(URL_REGEX.match(text))

def normalize_url(text: str) -> str:
    text = text.strip()
    if not text.startswith(("http://", "https://")):
        return "https://" + text
    return text

def html_to_text(html: str) -> str:
    try:
        soup = BeautifulSoup(html, "html.parser")
        for element in soup(["script", "style", "nav", "footer", "header", "aside"]):
            element.decompose()
        text = soup.get_text(separator="\n")
        return clean_text(text)
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML: {e}", exc_info=True)
        return ""

# =========================
# –ú–ï–¢–û–î–´ –ò–ó –°–¢–ê–¢–¨–ò –î–õ–Ø –û–ë–•–û–î–ê –ö–ê–ü–ß–ò –ò 403
# =========================

def _is_blocked_page(html: str) -> bool:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
    if not html or len(html) < 100:
        return True
    
    html_lower = html.lower()
    
    block_indicators = [
        "captcha",
        "cloudflare",
        "access denied",
        "–¥–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω",
        "403 forbidden",
        "are you human",
        "–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ —á—Ç–æ –≤—ã –Ω–µ —Ä–æ–±–æ—Ç",
        "security check",
        "ddos-guard",
    ]
    
    return any(indicator in html_lower for indicator in block_indicators)

def parse_with_cloudscraper(url: str, proxy_url: Optional[str] = None) -> str:
    """–ú–µ—Ç–æ–¥ –∏–∑ —Å—Ç–∞—Ç—å–∏: cloudscraper –¥–ª—è –æ–±—Ö–æ–¥–∞ Cloudflare"""
    try:
        import cloudscraper
        
        logger.info(f"‚òÅÔ∏è Cloudscraper: –ø–∞—Ä—Å–∏–º {url}")
        
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False
            }
        )
        
        proxies = None
        if proxy_url:
            normalized_proxy = _normalize_proxy_url(proxy_url)
            proxies = {'http': normalized_proxy, 'https': normalized_proxy}
        
        response = scraper.get(
            url, 
            headers=FULL_BROWSER_HEADERS,
            proxies=proxies,
            timeout=30
        )
        
        if response.status_code == 403:
            raise ValueError("–°–∞–π—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –¥–æ—Å—Ç—É–ø")
        
        response.raise_for_status()
        
        html = response.text
        
        if _is_blocked_page(html):
            raise ValueError("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
        
        logger.info(f"‚úÖ Cloudscraper —É—Å–ø–µ—à–µ–Ω: {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
        return html
        
    except ImportError:
        raise ImportError("Cloudscraper –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        logger.error(f"‚ùå Cloudscraper –æ—à–∏–±–∫–∞: {e}")
        raise

def parse_with_undetected_chromedriver(url: str, proxy_url: Optional[str] = None) -> str:
    """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∏–∑ —Å—Ç–∞—Ç—å–∏: undetected-chromedriver"""
    try:
        import undetected_chromedriver as uc
        
        logger.info(f"üõ°Ô∏è Undetected ChromeDriver: –ø–∞—Ä—Å–∏–º {url}")
        
        options = uc.ChromeOptions()
        
        if SELENIUM_HEADLESS:
            options.add_argument('--headless=new')
        
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-blink-features=AutomationControlled')
        
        if proxy_url:
            proxy_for_uc = _format_proxy_for_browser(proxy_url)
            options.add_argument(f'--proxy-server={proxy_for_uc}')
        
        driver = uc.Chrome(options=options, version_main=120, suppress_welcome=True)
        
        try:
            driver.execute_cdp_cmd('Network.setUserAgentOverride', {
                "userAgent": FULL_BROWSER_HEADERS["User-Agent"]
            })
            
            driver.execute_script("""
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            """)
            
            driver.get(url)
            time.sleep(random.uniform(3, 6))
            
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.5);")
            time.sleep(1)
            
            html = driver.page_source
            
            if _is_blocked_page(html):
                raise ValueError("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
            
            logger.info(f"‚úÖ Undetected ChromeDriver —É—Å–ø–µ—à–µ–Ω: {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
            return html
            
        finally:
            try:
                driver.quit()
            except:
                pass
                
    except ImportError:
        raise ImportError("Undetected ChromeDriver –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        logger.error(f"‚ùå Undetected ChromeDriver –æ—à–∏–±–∫–∞: {e}")
        raise

def parse_with_playwright(url: str, proxy_url: Optional[str] = None) -> str:
    """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∏–∑ —Å—Ç–∞—Ç—å–∏: Playwright"""
    try:
        from playwright.sync_api import sync_playwright
        
        logger.info(f"üé≠ Playwright: –ø–∞—Ä—Å–∏–º {url}")
        
        proxy_config = None
        if proxy_url:
            proxy_parts = _parse_proxy_url(proxy_url)
            proxy_config = {
                "server": f"http://{proxy_parts['host']}:{proxy_parts['port']}",
                "username": proxy_parts.get('username'),
                "password": proxy_parts.get('password')
            }
        
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=SELENIUM_HEADLESS,
                args=['--disable-blink-features=AutomationControlled', '--no-sandbox']
            )
            
            context = browser.new_context(
                user_agent=FULL_BROWSER_HEADERS["User-Agent"],
                viewport={'width': 1920, 'height': 1080},
                locale='ru-RU',
                proxy=proxy_config
            )
            
            page = context.new_page()
            
            try:
                page.goto(url, wait_until='networkidle', timeout=30000)
                page.wait_for_timeout(3000)
                
                html = page.content()
                
                if _is_blocked_page(html):
                    raise ValueError("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
                
                logger.info(f"‚úÖ Playwright —É—Å–ø–µ—à–µ–Ω: {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
                return html
                
            finally:
                try:
                    context.close()
                    browser.close()
                except:
                    pass
                
    except ImportError:
        raise ImportError("Playwright –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        logger.error(f"‚ùå Playwright –æ—à–∏–±–∫–∞: {e}")
        raise

# =========================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ü–†–û–ö–°–ò
# =========================

def _format_proxy_for_browser(proxy_url: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–∫—Å–∏ –¥–ª—è –±—Ä–∞—É–∑–µ—Ä–∞"""
    if proxy_url.startswith('http://'):
        proxy_url = proxy_url[7:]
    elif proxy_url.startswith('https://'):
        proxy_url = proxy_url[8:]
    
    if '@' in proxy_url:
        proxy_url = proxy_url.split('@')[1]
    
    return proxy_url

def _parse_proxy_url(proxy_url: str) -> Dict[str, Any]:
    """–ü–∞—Ä—Å–∏—Ç URL –ø—Ä–æ–∫—Å–∏"""
    result = {'host': '', 'port': '', 'username': None, 'password': None}
    
    try:
        if proxy_url.startswith('http://'):
            proxy_url = proxy_url[7:]
        
        if '@' in proxy_url:
            auth_part, host_part = proxy_url.split('@', 1)
            if ':' in auth_part:
                result['username'], result['password'] = auth_part.split(':', 1)
        else:
            host_part = proxy_url
        
        if ':' in host_part:
            result['host'], port_str = host_part.split(':', 1)
            result['port'] = int(port_str)
            
    except Exception:
        pass
    
    return result

# =========================
# –í–ê–® –°–£–©–ï–°–¢–í–£–Æ–©–ò–ô –ö–û–î (—Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏)
# =========================

def _normalize_proxy_url(raw: str) -> str:
    raw = (raw or "").strip()
    if not raw:
        return raw

    if re.match(r"^[a-zA-Z0-9+.-]+://", raw):
        return raw

    if "@" in raw:
        left, right = raw.split("@", 1)
        
        def looks_like_host_port(part: str) -> bool:
            host, _, _ = part.partition(":")
            return "." in host and re.search(r"[a-zA-Z]", host) is not None

        if looks_like_host_port(left):
            host_port = left
            creds = right
        else:
            creds = left
            host_port = right

        return f"http://{creds}@{host_port}"

    return f"http://{raw}"

def fetch_html_via_requests(url: str, proxy_url: Optional[str] = None) -> str:
    """–ó–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ requests (—É–ª—É—á—à–µ–Ω–Ω—ã–µ headers)"""
    proxies = None
    if proxy_url:
        normalized_proxy = _normalize_proxy_url(proxy_url)
        proxies = {"http": normalized_proxy, "https": normalized_proxy}
        logger.info(f"üîó Requests —Å –ø—Ä–æ–∫—Å–∏: {normalized_proxy}")

    try:
        logger.info(f"üåê Requests: –ø–∞—Ä—Å–∏–º {url}")
        
        session = requests.Session()
        session.headers.update(FULL_BROWSER_HEADERS)  # ‚Üê –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–µ headers –∏–∑ —Å—Ç–∞—Ç—å–∏
        
        resp = session.get(url, proxies=proxies, timeout=20)
        resp.raise_for_status()

        html = resp.text

        if _is_blocked_page(html):
            logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞/–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ (Requests)")
            raise ValueError(GENERIC_VACANCY_ERROR_MSG)

        if len(html) < 500:
            logger.warning(f"‚ö†Ô∏è –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç ({len(html)} —Å–∏–º–≤–æ–ª–æ–≤)")
            raise ValueError(GENERIC_VACANCY_ERROR_MSG)

        return html

    except requests.exceptions.Timeout:
        logger.error("‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ", exc_info=True)
        raise ValueError(GENERIC_VACANCY_ERROR_MSG)
    except requests.RequestException as e:
        logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {e}", exc_info=True)
        raise ValueError(GENERIC_VACANCY_ERROR_MSG)
    except Exception as e:
        logger.error(f"‚ùå –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ requests: {e}", exc_info=True)
        raise ValueError(GENERIC_VACANCY_ERROR_MSG)

# =========================
# –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–ê–†–°–ï–† –° –ú–ï–¢–û–î–ê–ú–ò –ò–ó –°–¢–ê–¢–¨–ò
# =========================

def fetch_url_text_via_proxy(url: str) -> str:
    """
    –£–º–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏:
    1. Cloudscraper (–±—ã—Å—Ç—Ä—ã–π –æ–±—Ö–æ–¥ Cloudflare)
    2. Undetected ChromeDriver (–ª—É—á—à–∏–π –¥–ª—è –∫–∞–ø—á–∏)
    3. Playwright (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
    4. Selenium (–≤–∞—à —Ç–µ–∫—É—â–∏–π)
    5. Requests —Å –ø—Ä–æ–∫—Å–∏
    6. Requests –±–µ–∑ –ø—Ä–æ–∫—Å–∏
    """
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π selenium –∫–æ–¥
    def parse_with_selenium_existing(url: str, proxy_url: Optional[str] = None) -> str:
        """–í–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π selenium –∫–æ–¥"""
        if not SELENIUM_ENABLED:
            raise ValueError(GENERIC_VACANCY_ERROR_MSG)
        
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.chrome.service import Service
            from webdriver_manager.chrome import ChromeDriverManager
            
            options = Options()
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            
            if SELENIUM_HEADLESS:
                options.add_argument('--headless=new')
            
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            options.add_argument('--user-agent=' + FULL_BROWSER_HEADERS["User-Agent"])
            
            if proxy_url:
                proxy_for_selenium = proxy_url
                if proxy_for_selenium.startswith('http://'):
                    proxy_for_selenium = proxy_for_selenium[7:]
                if '@' in proxy_for_selenium:
                    proxy_for_selenium = proxy_for_selenium.split('@')[1]
                options.add_argument(f'--proxy-server={proxy_for_selenium}')
            
            os.environ['WDM_LOG_LEVEL'] = '0'
            os.environ['WDM_LOCAL'] = '1'
            
            service = Service(ChromeDriverManager(cache_valid_range=30, path="/tmp/chromedriver").install())
            driver = webdriver.Chrome(service=service, options=options)
            
            driver.execute_script(
                "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
            )
            
            driver.get(url)
            time.sleep(random.uniform(3, 6))
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight * 0.5);")
            time.sleep(random.uniform(1, 2))
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(1, 2))
            
            html = driver.page_source
            driver.quit()
            
            if _is_blocked_page(html):
                raise ValueError(GENERIC_VACANCY_ERROR_MSG)
            
            return html
            
        except Exception as e:
            logger.error(f"‚ùå Selenium –æ—à–∏–±–∫–∞: {e}")
            raise ValueError(GENERIC_VACANCY_ERROR_MSG)
    
    # –ü–æ—Ä—è–¥–æ–∫ –ø–æ–ø—ã—Ç–æ–∫
    methods_to_try = []
    
    # 1. Cloudscraper (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
    if CLOUDSCRAPER_ENABLED:
        methods_to_try.append(("Cloudscraper", lambda: parse_with_cloudscraper(url, PROXY_URL)))
    
    # 2. Undetected ChromeDriver (–≥–ª–∞–≤–Ω—ã–π –∏–∑ —Å—Ç–∞—Ç—å–∏)
    methods_to_try.append(("Undetected ChromeDriver", lambda: parse_with_undetected_chromedriver(url, PROXY_URL)))
    
    # 3. Playwright (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
    if PLAYWRIGHT_ENABLED:
        methods_to_try.append(("Playwright", lambda: parse_with_playwright(url, PROXY_URL)))
    
    # 4. –í–∞—à Selenium
    if SELENIUM_ENABLED:
        methods_to_try.append(("Selenium", lambda: parse_with_selenium_existing(url, PROXY_URL)))
    
    # 5. Requests —Å –ø—Ä–æ–∫—Å–∏
    if PROXY_URL:
        methods_to_try.append(("Requests —Å –ø—Ä–æ–∫—Å–∏", lambda: fetch_html_via_requests(url, PROXY_URL)))
    
    # 6. Requests –±–µ–∑ –ø—Ä–æ–∫—Å–∏
    methods_to_try.append(("Requests –±–µ–∑ –ø—Ä–æ–∫—Å–∏", lambda: fetch_html_via_requests(url, None)))
    
    last_error = None
    
    for method_name, parser_func in methods_to_try:
        try:
            logger.info(f"üîÑ –ü—Ä–æ–±—É–µ–º {method_name} –¥–ª—è {url}")
            html = parser_func()
            text = html_to_text(html)
            
            if text and len(text) >= MIN_MEANINGFUL_TEXT_LENGTH:
                logger.info(f"‚úÖ {method_name} —É—Å–ø–µ—à–µ–Ω: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                return text
            else:
                logger.warning(f"‚ö†Ô∏è {method_name}: –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞")
                last_error = ValueError(GENERIC_VACANCY_ERROR_MSG)
                
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è {method_name} –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
            continue
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è {method_name} –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
            last_error = e
            continue
    
    logger.error("‚ùå –í—Å–µ –º–µ—Ç–æ–¥—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏")
    raise ValueError(GENERIC_VACANCY_ERROR_MSG)
