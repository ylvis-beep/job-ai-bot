import os
import logging
from typing import Any, Dict, List, cast, Optional
import re
import time
import random
import json
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
from io import BytesIO
from PyPDF2 import PdfReader

from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from openai import OpenAI
from openai.types.chat import ChatCompletionMessageParam

# Enable logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# Initialize OpenAI client
openai_client = None


class SmartParser:
    """–£–º–Ω—ã–π –ø–∞—Ä—Å–µ—Ä: —Å–Ω–∞—á–∞–ª–∞ –±–µ—Å–ø–ª–∞—Ç–Ω–æ, –ø—Ä–∏ 403 - ScrapingBee."""
    
    def __init__(self):
        # –ö–ª—é—á ScrapingBee –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        self.scrapingbee_key = os.getenv('SCRAPINGBEE_API_KEY', '')
        
        # –°–∞–π—Ç—ã, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –í–°–ï–ì–î–ê –ø—Ä–æ–±—É–µ–º ScrapingBee –ø—Ä–∏ 403
        self.priority_sites = [
            'tochka.com',
            'yandex.ru/jobs',
            '—è–Ω–¥–µ–∫—Å-—Ä–∞–±–æ—Ç–∞',
            'tinkoff.ru/career',
            'sber.ru/career',
            'vk.com/jobs',
            'vc.ru/jobs',
        ]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'direct_success': 0,
            'direct_403': 0,
            'direct_other_error': 0,
            'scrapingbee_success': 0,
            'scrapingbee_failed': 0,
            'total_requests': 0,
        }
    
    def should_try_scrapingbee(self, url: str, status_code: Optional[int] = None) -> bool:
        """
        –†–µ—à–∞–µ–º, —Å—Ç–æ–∏—Ç –ª–∏ –ø—Ä–æ–±–æ–≤–∞—Ç—å ScrapingBee.
        
        –ü—Ä–∞–≤–∏–ª–∞:
        1. –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–ª—é—á API
        2. –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å 403 –æ—à–∏–±–∫–∞ –ò–õ–ò —Å–∞–π—Ç –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–º —Å–ø–∏—Å–∫–µ
        3. –î–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –ø—Ä–æ–±—É–µ–º –¥–∞–∂–µ –ø—Ä–∏ –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–∫–∞—Ö
        """
        if not self.scrapingbee_key:
            logger.debug("No ScrapingBee API key available")
            return False
        
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –ª–∏ —Å–∞–π—Ç
        is_priority = any(site in domain for site in self.priority_sites)
        
        # –ï—Å–ª–∏ —Å–∞–π—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π - –ø—Ä–æ–±—É–µ–º ScrapingBee –ø—Ä–∏ –õ–Æ–ë–û–ô –æ—à–∏–±–∫–µ
        if is_priority and status_code is not None:
            logger.info(f"Priority site {domain}, will try ScrapingBee for error {status_code}")
            return True
        
        # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –ø—Ä–æ–±—É–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏ 403
        if status_code == 403:
            logger.info(f"403 error for {domain}, will try ScrapingBee")
            return True
        
        return False
    
    def parse(self, url: str) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –±–µ—Å–ø–ª–∞—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞."""
        logger.info(f"Smart parser starting for: {url}")
        self.stats['total_requests'] += 1
        
        # –®–ê–ì 1: –ü—Ä–æ–±—É–µ–º –ë–ï–°–ü–õ–ê–¢–ù–´–ô –ø—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥
        try:
            result = self._try_direct_parsing(url)
            self.stats['direct_success'] += 1
            logger.info(f"Direct parsing SUCCESS for {url}")
            return result
            
        except requests.HTTPError as e:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º HTTP –æ—à–∏–±–∫–∏
            status_code = e.response.status_code if hasattr(e, 'response') else None
            
            if status_code == 403:
                self.stats['direct_403'] += 1
                logger.warning(f"Direct parsing got 403 for {url}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—Ç–æ–∏—Ç –ª–∏ –ø—Ä–æ–±–æ–≤–∞—Ç—å ScrapingBee
                if self.should_try_scrapingbee(url, status_code):
                    return self._try_with_scrapingbee(url)
                else:
                    raise RuntimeError("DIRECT_403_NO_FALLBACK")
                    
            else:
                self.stats['direct_other_error'] += 1
                logger.error(f"Direct parsing HTTP error {status_code} for {url}")
                
                # –î–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –ø—Ä–æ–±—É–µ–º ScrapingBee –¥–∞–∂–µ –ø—Ä–∏ –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–∫–∞—Ö
                if self.should_try_scrapingbee(url, status_code):
                    return self._try_with_scrapingbee(url)
                else:
                    raise RuntimeError(f"DIRECT_HTTP_ERROR_{status_code}")
                    
        except Exception as e:
            self.stats['direct_other_error'] += 1
            logger.error(f"Direct parsing general error for {url}: {e}")
            
            # –î–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –ø—Ä–æ–±—É–µ–º ScrapingBee –¥–∞–∂–µ –ø—Ä–∏ –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–∫–∞—Ö
            if self.should_try_scrapingbee(url):
                return self._try_with_scrapingbee(url)
            else:
                raise RuntimeError("DIRECT_GENERAL_ERROR")
    
    def _try_direct_parsing(self, url: str) -> str:
        """–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π –ø—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏."""
        headers = self._get_realistic_headers()
        
        # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ 1-3 —Å–µ–∫—É–Ω–¥—ã (–∏–º–∏—Ç–∞—Ü–∏—è —á–µ–ª–æ–≤–µ–∫–∞)
        time.sleep(random.uniform(1, 3))
        
        logger.info(f"Trying DIRECT parsing for: {url}")
        
        response = requests.get(
            url, 
            headers=headers, 
            timeout=25,
            allow_redirects=True,
            verify=True
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å
        response.raise_for_status()  # –í—ã–∑–æ–≤–µ—Ç HTTPError –µ—Å–ª–∏ —Å—Ç–∞—Ç—É—Å –Ω–µ 2xx
        
        # –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–∏–ª–∏ –æ—Ç–≤–µ—Ç
        logger.info(f"Direct parsing SUCCESS: {response.status_code}")
        
        # –ü–∞—Ä—Å–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç
        return self._parse_html_content(response.text, url, "direct")
    
    def _try_with_scrapingbee(self, url: str) -> str:
        """–ò—Å–ø–æ–ª—å–∑—É–µ–º ScrapingBee API –∫–∞–∫ fallback."""
        if not self.scrapingbee_key:
            raise RuntimeError("SCRAPINGBEE_NO_KEY")
        
        logger.info(f"Trying ScrapingBee for: {url}")
        
        api_url = "https://app.scrapingbee.com/api/v1/"
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
        params = {
            'api_key': self.scrapingbee_key,
            'url': url,
            'render_js': 'true',        # –í–ö–õ–Æ–ß–ê–ï–ú JavaScript —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥!
            'premium_proxy': 'true',    # –ü—Ä–µ–º–∏—É–º –ø—Ä–æ–∫—Å–∏ (–ª—É—á—à–µ –æ–±—Ö–æ–¥)
            'country_code': 'ru',       # –†–æ—Å—Å–∏–π—Å–∫–∏–µ –ø—Ä–æ–∫—Å–∏
            'wait': '3000',             # –ñ–¥–µ–º 3 —Å–µ–∫—É–Ω–¥—ã –¥–ª—è JS
            'wait_for': '3000',         # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –æ–∂–∏–¥–∞–Ω–∏—è
            'timeout': '30000',         # –¢–∞–π–º–∞—É—Ç 30 —Å–µ–∫—É–Ω–¥
        }
        
        # –î–ª—è tochka.com –¥–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if 'tochka.com' in url:
            params.update({
                'stealth_proxy': 'true',  # –°—Ç–µ–ª—Å-–ø—Ä–æ–∫—Å–∏ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
                'session_id': str(int(time.time())),  # –£–Ω–∏–∫–∞–ª—å–Ω–∞—è —Å–µ—Å—Å–∏—è
            })
        
        try:
            # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ ScrapingBee
            response = requests.get(
                api_url, 
                params=params, 
                timeout=35,  # –ë–æ–ª—å—à–æ–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è JS —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            )
            
            logger.info(f"ScrapingBee response: {response.status_code}")
            
            if response.status_code != 200:
                error_msg = f"ScrapingBee error {response.status_code}"
                if response.text:
                    error_msg += f": {response.text[:200]}"
                logger.error(error_msg)
                
                self.stats['scrapingbee_failed'] += 1
                raise RuntimeError("SCRAPINGBEE_API_ERROR")
            
            # –£—Å–ø–µ—Ö!
            self.stats['scrapingbee_success'] += 1
            
            # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            return self._parse_html_content(response.text, url, "scrapingbee")
            
        except requests.Timeout:
            logger.error("ScrapingBee timeout")
            self.stats['scrapingbee_failed'] += 1
            raise RuntimeError("SCRAPINGBEE_TIMEOUT")
            
        except requests.RequestException as e:
            logger.error(f"ScrapingBee request error: {e}")
            self.stats['scrapingbee_failed'] += 1
            raise RuntimeError("SCRAPINGBEE_NETWORK_ERROR")
            
        except Exception as e:
            logger.error(f"ScrapingBee unexpected error: {e}")
            self.stats['scrapingbee_failed'] += 1
            raise RuntimeError("SCRAPINGBEE_UNKNOWN_ERROR")
    
    def _get_realistic_headers(self) -> Dict[str, str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –±—Ä–∞—É–∑–µ—Ä–∞."""
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:123.0) Gecko/20100101 Firefox/123.0",
        ]
        
        headers = {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'Pragma': 'no-cache',
        }
        
        return headers
    
    def _parse_html_content(self, html: str, url: str, source: str) -> str:
        """–ü–∞—Ä—Å–∏–º HTML –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –ª—é–±–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞."""
        logger.info(f"Parsing HTML from {source} for {url}")
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for tag in soup(['script', 'style', 'noscript', 'iframe', 'svg', 'link', 'meta']):
                tag.decompose()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            json_ld_text = self._extract_json_ld(soup)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            meta_text = self._extract_meta_info(soup)
            
            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content_text = self._find_main_content(soup)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            result_parts = []
            
            if json_ld_text:
                result_parts.append(f"üìã –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:\n{json_ld_text}")
            
            if meta_text:
                result_parts.append(f"üè∑Ô∏è –ú–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n{meta_text}")
            
            if content_text:
                result_parts.append(f"üìÑ –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç:\n{content_text}")
            
            if result_parts:
                result = "\n\n".join(result_parts)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                if len(result.strip()) > 150:
                    logger.info(f"Successfully extracted {len(result)} chars from {source}")
                    return self._clean_text(result)
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –≤–µ—Å—å —Ç–µ–∫—Å—Ç
            full_text = soup.get_text(separator='\n', strip=True)
            cleaned = self._clean_text(full_text)
            
            if len(cleaned.strip()) > 100:
                logger.info(f"Using full text: {len(cleaned)} chars")
                return cleaned
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
            logger.warning(f"Extracted text too short from {source}: {len(cleaned)} chars")
            raise RuntimeError("CONTENT_TOO_SHORT")
            
        except Exception as e:
            logger.error(f"Error parsing HTML from {source}: {e}")
            raise RuntimeError("HTML_PARSING_ERROR")
    
    def _extract_json_ld(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ."""
        try:
            scripts = soup.find_all('script', type='application/ld+json')
            results = []
            
            for script in scripts:
                try:
                    data = json.loads(script.string)
                    if isinstance(data, dict):
                        # –ò—â–µ–º JobPosting
                        if data.get('@type') == 'JobPosting':
                            job_info = []
                            for key in ['title', 'description', 'responsibilities', 
                                       'requirements', 'qualifications', 'skills']:
                                if key in data and data[key]:
                                    value = data[key]
                                    if isinstance(value, list):
                                        job_info.append(f"{key}: " + ", ".join(str(v) for v in value))
                                    else:
                                        job_info.append(f"{key}: {value}")
                            
                            if job_info:
                                results.append("\n".join(job_info))
                except:
                    continue
            
            return "\n\n".join(results) if results else ""
        except:
            return ""
    
    def _extract_meta_info(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."""
        meta_parts = []
        
        # Title
        title = soup.find('title')
        if title and title.string:
            meta_parts.append(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title.string.strip()}")
        
        # H1
        h1_tags = soup.find_all('h1')
        for h1 in h1_tags[:2]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2 h1
            if h1 and h1.get_text(strip=True):
                meta_parts.append(f"H1: {h1.get_text(strip=True)}")
        
        # Meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            meta_parts.append(f"–û–ø–∏—Å–∞–Ω–∏–µ: {meta_desc['content'].strip()}")
        
        # Open Graph
        og_desc = soup.find('meta', attrs={'property': 'og:description'})
        if og_desc and og_desc.get('content'):
            meta_parts.append(f"OG –û–ø–∏—Å–∞–Ω–∏–µ: {og_desc['content'].strip()}")
        
        return "\n".join(meta_parts) if meta_parts else ""
    
    def _find_main_content(self, soup: BeautifulSoup) -> str:
        """–ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        content_selectors = [
            # –í–∞–∫–∞–Ω—Å–∏–∏
            '[data-qa*="vacancy"]', '[data-test*="vacancy"]', '[data-qa*="description"]',
            '[class*="vacancy" i]', '[class*="job" i]', '[class*="description" i]',
            '[itemtype*="JobPosting"]', '[itemprop="description"]',
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            'article', 'main', '[role="main"]',
            '[class*="content" i]', '[class*="text" i]', '[class*="body" i]',
            '.container', '.wrapper', '.page-content',
            
            # –û–±—â–∏–µ
            'section', '.post-content', '.article-content',
        ]
        
        for selector in content_selectors:
            try:
                elements = soup.select(selector)
                for elem in elements:
                    text = elem.get_text(separator='\n', strip=True)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é
                    if (len(text) > 300 and 
                        any(word in text.lower() for word in 
                            ['—Ç—Ä–µ–±–æ–≤–∞–Ω', '–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç', '–∑–∞–¥–∞—á', '–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü', '–æ–ø—ã—Ç', '–Ω–∞–≤—ã–∫'])):
                        return text
                    elif len(text) > 500:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                        return text
            except Exception as e:
                logger.debug(f"Selector {selector} failed: {e}")
                continue
        
        return ""
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç."""
        if not text:
            return ""
        
        # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'[ \t]{2,}', ' ', text)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        # –£–¥–∞–ª—è–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏ –µ—Å–ª–∏ –∏—Ö –º–Ω–æ–≥–æ
        if len(lines) > 50:
            lines = [line for line in lines if len(line) > 20]
        
        return '\n'.join(lines)
    
    def get_stats(self) -> Dict[str, int]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
        return self.stats.copy()
    
    def print_stats(self):
        """–í—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –ª–æ–≥."""
        stats = self.get_stats()
        logger.info("=== Parser Statistics ===")
        for key, value in stats.items():
            logger.info(f"{key}: {value}")


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä
smart_parser = SmartParser()


def load_system_prompt() -> str:
    """Load the system prompt from system_prompt.txt if it exists."""
    try:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(base_dir, "system_prompt.txt")
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return "You are a helpful assistant."
    except Exception as e:
        logger.warning(f"Failed to load system_prompt.txt: {e}")
        return "You is a helpful assistant."


# =========================
# –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# =========================

def clean_text(raw: str) -> str:
    """–ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –≤ –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–π –≤–∏–¥."""
    if not raw:
        return ""
    text = raw.replace("\r\n", "\n")
    lines = [line.strip() for line in text.split("\n")]
    text = "\n".join(lines)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()


def is_url(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–∞ –ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–∞ URL."""
    if not text:
        return False
    text = text.strip()
    
    if re.match(r'^https?://\S+$', text, re.IGNORECASE):
        return True
    
    try:
        parsed = urlparse(text)
        return bool(parsed.netloc) and '.' in parsed.netloc
    except:
        return False


def extract_text_from_pdf_bytes(data: bytes) -> str:
    """–î–æ—Å—Ç–∞—ë–º —Ç–µ–∫—Å—Ç –∏–∑ PDF –ø–æ —Å—ã—Ä—ã–º –±–∞–π—Ç–∞–º."""
    try:
        reader = PdfReader(BytesIO(data))
        pages_text: List[str] = []
        for page in reader.pages:
            page_text = page.extract_text() or ""
            pages_text.append(page_text)
        return clean_text("\n\n".join(pages_text))
    except Exception as e:
        logger.error(f"Error while extracting text from PDF bytes: {e}")
        return ""


def extract_text_from_url(url: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ URL –∏—Å–ø–æ–ª—å–∑—É—è —É–º–Ω—ã–π –ø–∞—Ä—Å–µ—Ä."""
    logger.info(f"Extracting from URL: {url}")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    try:
        return smart_parser.parse(url)
        
    except RuntimeError as e:
        error_type = str(e)
        logger.warning(f"Smart parser failed: {error_type}")
        raise e
        
    except Exception as e:
        logger.error(f"Unexpected error in extract_text_from_url: {e}")
        raise RuntimeError("UNKNOWN_ERROR")


def prepare_input_text(raw: str) -> str:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    –í—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç, –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –ø–∞–¥–∞–µ–º —Å –æ—à–∏–±–∫–æ–π.
    """
    if not raw:
        return ""
    
    raw = raw.strip()
    
    # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ —Å—Å—ã–ª–∫–∞, –ø—Ä–æ—Å—Ç–æ —á–∏—Å—Ç–∏–º —Ç–µ–∫—Å—Ç
    if not is_url(raw):
        return clean_text(raw)
    
    # –ï—Å–ª–∏ —ç—Ç–æ —Å—Å—ã–ª–∫–∞ - –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å
    logger.info(f"Processing URL: {raw}")
    
    try:
        text = extract_text_from_url(raw)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if text and len(text.strip()) > 150:
            return text
        else:
            # –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
            return clean_text(f"–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é: {raw}")
            
    except RuntimeError as e:
        error_type = str(e)
        logger.info(f"Parser failed with: {error_type}")
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –æ—à–∏–±–æ–∫
        if "403" in error_type or "DIRECT_403" in error_type:
            if 'tochka.com' in raw.lower():
                return clean_text(f"üîí tochka.com (–∑–∞—â–∏—â–µ–Ω–Ω—ã–π —Å–∞–π—Ç)\n–°—Å—ã–ª–∫–∞: {raw}\n\n–î–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏.")
            else:
                return clean_text(f"üîí –°–∞–π—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –¥–æ—Å—Ç—É–ø\n–°—Å—ã–ª–∫–∞: {raw}\n\n–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏.")
        
        elif "SCRAPINGBEE" in error_type:
            return clean_text(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏\n–°—Å—ã–ª–∫–∞: {raw}\n\n–°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –≤—Ä—É—á–Ω—É—é.")
        
        else:
            return clean_text(f"–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é: {raw}")
            
    except Exception as e:
        logger.error(f"Unexpected error in prepare_input_text: {e}")
        return clean_text(f"–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é: {raw}")


# =========================
# Telegram Handlers
# =========================

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start."""
    message = update.message
    if message is None:
        return

    user = update.effective_user
    welcome_text = (
        "üëã *–ü—Ä–∏–≤–µ—Ç! –Ø —Ç–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫ –≤ –ø–æ–∏—Å–∫–µ —Ä–∞–±–æ—Ç—ã.*\n\n"
        "–Ø –ø–æ–º–æ–≥—É:\n"
        "‚Ä¢ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∞–∫–∞–Ω—Å–∏–∏\n"
        "‚Ä¢ –°–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å —Å –≤–∞—à–∏–º —Ä–µ–∑—é–º–µ\n"
        "‚Ä¢ –°–æ—Å—Ç–∞–≤–∏—Ç—å —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∏—Å—å–º–∞\n"
        "‚Ä¢ –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å—Å—è –∫ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è–º\n\n"
        "üì§ *–û—Ç–ø—Ä–∞–≤—å –º–Ω–µ:*\n"
        "‚úÖ –¢–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ –∏–ª–∏ PDF\n"
        "‚úÖ –°—Å—ã–ª–∫—É –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é\n"
        "‚úÖ –¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏\n\n"
        "–Ø —Ä–∞–±–æ—Ç–∞—é —Å –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ–º —Å–∞–π—Ç–æ–≤! üöÄ"
    )
    
    await message.reply_text(welcome_text, parse_mode='Markdown')


async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /help."""
    message = update.message
    if message is None:
        return

    help_text = (
        "üìñ *–ü–æ–º–æ—â—å –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –±–æ—Ç–∞*\n\n"
        "*–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:*\n"
        "/start - –ù–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã\n"
        "/help - –≠—Ç–∞ —Å–ø—Ä–∞–≤–∫–∞\n"
        "/update_resume - –û–±–Ω–æ–≤–∏—Ç—å —Ä–µ–∑—é–º–µ\n"
        "/stats - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞\n\n"
        "*–ß—Ç–æ –º–æ–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å:*\n"
        "1. –¢–µ–∫—Å—Ç –≤–∞—à–µ–≥–æ —Ä–µ–∑—é–º–µ\n"
        "2. PDF —Ñ–∞–π–ª —Å —Ä–µ–∑—é–º–µ\n"
        "3. –°—Å—ã–ª–∫—É –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é\n"
        "4. –¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏\n\n"
        "*–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Å–∞–π—Ç—ã:*\n"
        "‚Ä¢ hh.ru, habr.com/career\n"
        "‚Ä¢ linkedin.com, moikrug.ru\n"
        "‚Ä¢ rabota.ru, superjob.ru\n"
        "‚Ä¢ –∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –¥—Ä—É–≥–∏—Ö\n\n"
        "*–ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç—Å—è:*\n"
        "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏\n"
        "–∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –µ–≥–æ —Ç–µ–∫—Å—Ç–æ–º."
    )
    
    await message.reply_text(help_text, parse_mode='Markdown')


async def stats_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞—Ä—Å–∏–Ω–≥–∞."""
    stats = smart_parser.get_stats()
    
    stats_text = (
        "üìä *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞*\n\n"
        f"‚úÖ –ü—Ä—è–º–æ–π —É—Å–ø–µ—à–Ω—ã–π: {stats['direct_success']}\n"
        f"üîí –ü—Ä—è–º–æ–π 403 –æ—à–∏–±–æ–∫: {stats['direct_403']}\n"
        f"‚ö†Ô∏è –ü—Ä—è–º–æ–π –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫: {stats['direct_other_error']}\n"
        f"üí∞ ScrapingBee —É—Å–ø–µ—à–Ω—ã–π: {stats['scrapingbee_success']}\n"
        f"üí∏ ScrapingBee –Ω–µ—É–¥–∞—á–Ω—ã–π: {stats['scrapingbee_failed']}\n"
        f"üìà –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['total_requests']}\n\n"
        f"*–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å:*\n"
        f"–ë–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ: {stats['direct_success']}/{stats['total_requests']}\n"
        f"–ü–ª–∞—Ç–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {stats['scrapingbee_success'] + stats['scrapingbee_failed']}"
    )
    
    await update.message.reply_text(stats_text, parse_mode='Markdown')


async def update_resume(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /update_resume."""
    message = update.message
    if message is None:
        return

    user_data = cast(Dict[str, Any], context.user_data)
    user_data['awaiting_resume'] = True
    user_data.pop('resume', None)
    
    await message.reply_text(
        "üìù *–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—é–º–µ*\n\n"
        "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –Ω–æ–≤–æ–µ —Ä–µ–∑—é–º–µ (PDF –∏–ª–∏ —Ç–µ–∫—Å—Ç).\n"
        "–Ø –æ–±–Ω–æ–≤–ª—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∞—à–µ–º –æ–ø—ã—Ç–µ.",
        parse_mode='Markdown'
    )


async def chat(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π."""
    message = update.message
    if message is None:
        return

    if not openai_client:
        await message.reply_text(
            "‚ö†Ô∏è *–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏*\n\n"
            "OpenAI –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OPENAI_API_KEY.",
            parse_mode='Markdown'
        )
        return

    user_data = cast(Dict[str, Any], context.user_data)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞
    user_message: str

    if message.document is not None:
        doc = message.document
        is_pdf = (
            doc.mime_type == "application/pdf"
            or (doc.file_name and doc.file_name.lower().endswith(".pdf"))
        )

        if not is_pdf:
            await message.reply_text(
                "‚ö†Ô∏è *–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç*\n\n"
                "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ PDF —Ñ–∞–π–ª—ã –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è.",
                parse_mode='Markdown'
            )
            return

        try:
            file = await doc.get_file()
            bio = BytesIO()
            await file.download_to_memory(out=bio)
            pdf_bytes = bio.getvalue()
            extracted = extract_text_from_pdf_bytes(pdf_bytes)
            
            if not extracted:
                await message.reply_text(
                    "‚ùå *–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç*\n\n"
                    "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π PDF —Ñ–∞–π–ª –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç.",
                    parse_mode='Markdown'
                )
                return
                
            user_message = extracted
        except Exception as e:
            logger.error(f"PDF error: {e}")
            await message.reply_text(
                "‚ùå *–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF*\n\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç.",
                parse_mode='Markdown'
            )
            return

    else:  # –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        raw_text = message.text
        if raw_text is None:
            await message.reply_text(
                "‚ö†Ô∏è *–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç*\n\n"
                "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –∏–ª–∏ PDF —Ñ–∞–π–ª—ã.",
                parse_mode='Markdown'
            )
            return
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –Ω–∞–±–æ—Ä–∞
        await message.chat.send_action(action="typing")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç (–≤–∫–ª—é—á–∞—è —Å—Å—ã–ª–∫–∏)
        user_message = prepare_input_text(raw_text)

    # –ï—Å–ª–∏ –æ–∂–∏–¥–∞–µ–º –Ω–æ–≤–æ–µ —Ä–µ–∑—é–º–µ
    if user_data.get("awaiting_resume"):
        user_data["resume"] = user_message
        user_data["awaiting_resume"] = False
        
        await message.reply_text(
            "‚úÖ *–†–µ–∑—é–º–µ –æ–±–Ω–æ–≤–ª–µ–Ω–æ!*\n\n"
            "–¢–µ–ø–µ—Ä—å –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –≤–∞–∫–∞–Ω—Å–∏—é –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.\n"
            "–Ø –±—É–¥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ —Ä–µ–∑—é–º–µ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç.",
            parse_mode='Markdown'
        )
        return

    try:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        await message.chat.send_action(action="typing")

        system_prompt = load_system_prompt()

        # –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞
        history = cast(List[Dict[str, str]], user_data.get("history", []))
        max_history_messages = 10

        messages: List[ChatCompletionMessageParam] = [
            {
                "role": "system",
                "content": system_prompt
            }
        ]

        if history:
            messages.extend(history[-max_history_messages:])

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—é–º–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        resume = user_data.get("resume")
        if resume:
            messages.append({
                "role": "user",
                "content": (
                    "–≠—Ç–æ —Ä–µ–∑—é–º–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ò—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç "
                    "–ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≤–∞–∫–∞–Ω—Å–∏–π, —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ —Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∏—Å–µ–º:\n\n"
                    f"{resume}"
                ),
            })

        # –¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        messages.append({
            "role": "user",
            "content": user_message
        })

        # –í—ã–∑–æ–≤ OpenAI API
        response = openai_client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=messages,
            max_completion_tokens=2048
        )

        ai_response = response.choices[0].message.content or (
            "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."
        )
        
        await message.reply_text(ai_response)

        # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        if ai_response:
            history.append({"role": "user", "content": user_message})
            history.append({"role": "assistant", "content": ai_response})
            user_data["history"] = history[-max_history_messages:]

    except Exception as e:
        logger.error(f"OpenAI API error: {e}")
        await message.reply_text(
            "‚ùå *–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞*\n\n"
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –ø–æ–∑–∂–µ.",
            parse_mode='Markdown'
        )


async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫."""
    logger.error(f"Update error: {context.error}")
    
    # –õ–æ–≥–∏—Ä—É–µ–º, –Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º


def main() -> None:
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞."""
    global openai_client

    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã
    token = os.getenv('TELEGRAM_BOT_TOKEN')
    openai_api_key = os.getenv('OPENAI_API_KEY')
    scrapingbee_key = os.getenv('SCRAPINGBEE_API_KEY')

    if not token:
        logger.error("TELEGRAM_BOT_TOKEN not found!")
        print("ERROR: Set TELEGRAM_BOT_TOKEN environment variable")
        return

    if not openai_api_key:
        logger.warning("OPENAI_API_KEY not found - AI features disabled")
        print("WARNING: Set OPENAI_API_KEY to enable AI")
    else:
        openai_client = OpenAI(api_key=openai_api_key)
        logger.info("OpenAI client initialized")

    if scrapingbee_key:
        logger.info("ScrapingBee API key found - premium parsing enabled")
    else:
        logger.info("ScrapingBee API key not found - only free parsing")

    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
    application = Application.builder().token(token).build()

    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("update_resume", update_resume))
    application.add_handler(CommandHandler("stats", stats_command))

    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
    application.add_handler(
        MessageHandler(
            (filters.TEXT | filters.Document.PDF) & ~filters.COMMAND,
            chat,
        )
    )

    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫
    application.add_error_handler(error_handler)

    # –ó–∞–ø—É—Å–∫–∞–µ–º
    logger.info("Bot starting...")
    print("‚úÖ Bot is running! Press Ctrl+C to stop.")
    application.run_polling(allowed_updates=Update.ALL_TYPES)


if __name__ == '__main__':
    main()
